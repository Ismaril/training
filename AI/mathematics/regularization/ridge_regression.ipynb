{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RIDGE REGRESSION COST FUNCTION\n",
    "\n",
    "Ridge Regression, also known as Tikhonov regularization, is a type of linear regression that includes a regularization term. The purpose of this regularization term is to penalize large coefficients in the model, which can help prevent overfitting and improve the model's generalization to new data. The cost function for Ridge Regression combines the Residual Sum of Squares (RSS) with a penalty on the size of the coefficients.\n",
    "\n",
    "The Ridge Regression cost function is given by:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{m} \\theta_j^2$\n",
    "\n",
    "where:\n",
    "- $J(\\theta)$ is the cost function to be minimized.\n",
    "- $n$ is the number of observations.\n",
    "- $y_i$ is the actual value for the \\(i\\)-th observation.\n",
    "- $\\hat{y}_i$ is the predicted value for the \\(i\\)-th observation, which is calculated as $\\hat{y}_i = \\theta^T x_i$ where $\\theta$ is the coefficient vector and $x_i$ is the feature vector for the \\(i\\)-th observation.\n",
    "- $\\lambda$ is the regularization parameter, a hyperparameter that controls the amount of shrinkage: the larger the value of $\\lambda$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n",
    "- $\\theta_j$ are the model coefficients, and $m$ is the number of features (excluding the intercept).\n",
    "- The first term is the RSS divided by $2n$, which normalizes the RSS by the number of observations.\n",
    "- The second term is the regularization term, where $\\lambda \\sum_{j=1}^{m} \\theta_j^2$ adds a penalty for large coefficients.\n",
    "\n",
    "### Practical Example with Ridge Regression Cost Function\n",
    "\n",
    "Let's consider a simple example with a dataset containing only 2 observations and 1 feature to illustrate the computation of the Ridge Regression cost function.\n",
    "\n",
    "Suppose we have the following dataset:\n",
    "\n",
    "| Observation | Feature $x_1$ | Actual Value $y$ |\n",
    "|-------------|---------------|------------------|\n",
    "| 1           | 1             | 2                |\n",
    "| 2           | 2             | 3                |\n",
    "\n",
    "And we have a simple Ridge Regression model with the following parameters:\n",
    "\n",
    "- $\\theta_0 = 0.5$ (intercept)\n",
    "- $\\theta_1 = 1$ (coefficient for $x_1$)\n",
    "- $\\lambda = 0.1$\n",
    "\n",
    "Our model predicts $\\hat{y}$ as:\n",
    "\n",
    "$\\hat{y} = \\theta_0 + \\theta_1 x_1$\n",
    "\n",
    "Let's compute the cost function $J(\\theta)$ for this model and dataset.\n",
    "\n",
    "### Step 1: Compute Predicted Values\n",
    "Calculate the predicted values $\\hat{y}$ for each observation.\n",
    "\n",
    "For Observation 1: $\\hat{y}_1 = 0.5 + 1 \\times 1 = 1.5$  \n",
    "For Observation 2: $\\hat{y}_2 = 0.5 + 1 \\times 2 = 2.5$\n",
    "\n",
    "### Step 2: Compute RSS\n",
    "Calculate the Residual Sum of Squares (RSS).\n",
    "\n",
    "RSS = $(2 - 1.5)^2 + (3 - 2.5)^2 = 0.25 + 0.25 = 0.5$\n",
    "\n",
    "### Step 3: Compute Regularization Term\n",
    "Calculate the regularization term.\n",
    "\n",
    "Regularization Term = $\\lambda \\sum_{j=1}^{m} \\theta_j^2 = 0.1 \\times 1^2 = 0.1$\n",
    "\n",
    "### Step 4: Compute Cost Function\n",
    "Combine the RSS and the regularization term to get the cost function $J(\\theta)$.\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2 \\times 2} \\times 0.5 + 0.1 = 0.125 + 0.1 = 0.225$\n",
    "\n",
    "### Interpretation\n",
    "The cost function value of 0.225 represents the cost associated with our Ridge Regression model given the dataset and the chosen $\\lambda$. It includes both the error in prediction (RSS) and the penalty for the size of the coefficient. Minimizing this cost function during training helps in finding the coefficients that not only fit the training data well but are also kept relatively small to avoid overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87df57943600247f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the Ridge Regression cost function $J(\\theta)$ using actual numbers with a dataset that has two features $x_1$ and $x_2$ and each feature has two observations. We'll follow the formula:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{m} \\theta_j^2$\n",
    "\n",
    "### Given Dataset:\n",
    "Assume our dataset consists of the following:\n",
    "\n",
    "| $x_1$ (Feature 1) | $x_2$ (Feature 2) | $y$ (Actual Value) |\n",
    "|-------------------|-------------------|--------------------|\n",
    "| 1                 | 3                 | 2                  |\n",
    "| 2                 | 4                 | 3                  |\n",
    "\n",
    "### Model Parameters:\n",
    "Let's assume the Ridge Regression model parameters are:\n",
    "- Intercept $\\theta_0 = 0.5$\n",
    "- Coefficient for $x_1$, $\\theta_1 = 1$\n",
    "- Coefficient for $x_2$, $\\theta_2 = 0.5$\n",
    "- Regularization parameter $\\lambda = 0.1$\n",
    "\n",
    "### Steps to Compute $J(\\theta)$:\n",
    "\n",
    "#### Step 1: Compute Predicted Values $\\hat{y}_i$\n",
    "$\\hat{y}_i = \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2}$ (In case of linear regression) \\\n",
    "For Observation 1: $\\hat{y}_1 = 0.5 + 1 \\times 1 + 0.5 \\times 3 = 2.5$  \n",
    "For Observation 2: $\\hat{y}_2 = 0.5 + 1 \\times 2 + 0.5 \\times 4 = 4$\n",
    "\n",
    "#### Step 2: Compute Sum of Squared Residuals\n",
    "$\\sum_{i=1}^{2} (y_i - \\hat{y}_i)^2 = (2 - 2.5)^2 + (3 - 4)^2$\n",
    "\n",
    "#### Step 3: Compute Regularization Term\n",
    "$\\lambda \\sum_{j=1}^{2} \\theta_j^2 = 0.1 \\times (1^2 + 0.5^2)$\n",
    "\n",
    "#### Step 4: Compute $J(\\theta)$\n",
    "$J(\\theta) = \\frac{1}{2 \\times 2} \\left( (2 - 2.5)^2 + (3 - 4)^2 \\right) + 0.1 \\times (1^2 + 0.5^2)$\n",
    "\n",
    "Let's perform these calculations to find the value of $J(\\theta)$.\n",
    "\n",
    "The computed value of the Ridge Regression cost function $J(\\theta)$ for the given dataset, with two features and two observations per feature, is $0.9375$. This cost value encapsulates both the model's prediction error (sum of squared residuals) and the penalty imposed on the size of the coefficients (regularization term), as dictated by the Ridge Regression framework. The regularization parameter $\\lambda = 0.1$ in this case helps to control the magnitude of the coefficients, thereby aiming to reduce overfitting."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb36eff278a0efaa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For info, sometimes you might se 1/2. Both formulations are mathematically valid and will lead to the same set of coefficients after optimization, although the effective value of $\\lambda$ may need to be adjusted between the two formulations to achieve the same level of regularization. The choice of including the 1/2 factor is mostly a matter of convenience for simplifying calculus operations and does not impact the goal of regularization, which is to penalize large coefficients to improve model generalization.\n",
    "\n",
    "$\\lambda \\sum_{j=1}^{m} \\theta_j^2$ \\\n",
    "$\\frac{1}{2} \\lambda \\sum_{j=1}^{m} \\theta_j^2$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "927ea91e48d51d28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CLOSED FORM SOLUTION\n",
    "\n",
    "Ridge regression extends linear regression by adding a regularization term to the cost function, which penalizes large coefficients to prevent overfitting. The closed-form solution for Ridge regression, also known as the Ridge regression normal equation, is given by:\n",
    "\n",
    "$\\hat{\\theta} = ((X^T \\cdot X)+ \\lambda \\cdot I)^{-1} \\cdot (X^T \\cdot y) $\n",
    "\n",
    "where:\n",
    "- $\\hat{\\theta}$ - is the vector of coefficients, meaning estimated model parameters that minimize the cost function.\n",
    "- $X$ - is the matrix of feature values, with each row representing an observation and each column a feature. An additional column of ones is typically added to $X$ to include the intercept term.\n",
    "- $y$ - is the target/labels vector.\n",
    "- $\\lambda$ - is the regularization parameter, a non-negative value that controls the strength of the regularization. Larger values of $\\lambda$ impose a greater penalty on the size of the coefficients.\n",
    "- $I$ - is the identity matrix, with the same number of rows and columns as the number of features (including the intercept). The first diagonal element is often set to 0 to exclude the intercept from regularization.\n",
    "- $X^T$ - is the transpose of $X$.\n",
    "- $(X^T X + \\lambda I)^{-1}$ - is the inverse of $X^T X + \\lambda I$.\n",
    "- $X^T y$ - is the matrix multiplication of $X^T$ and $y$.\n",
    " \n",
    "\n",
    "\n",
    "To explain how the Ridge regression closed-form formula works step by step with the given dataset, let's go through the process:\n",
    "\n",
    "### Given Dataset:\n",
    "\n",
    "| $x_1$   | $x_2$   | $y$   |\n",
    "|---------|---------|-------|\n",
    "| 1       | 2       | 3     |\n",
    "| 4       | 5       | 6     |\n",
    "\n",
    "### Step 1: Augment the Feature Matrix $X$\n",
    "\n",
    "First, we need to create the feature matrix $X$ and include a column of ones for the intercept term.\n",
    "\n",
    "$X = \\begin{bmatrix} 1 & 1 & 2 \\\\ 1 & 4 & 5 \\end{bmatrix} $\n",
    "\n",
    "### Step 2: Define the Target Vector $y$\n",
    "\n",
    "The target vector $y$ consists of the output values.\n",
    "\n",
    "$y = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} $\n",
    "\n",
    "### Step 3: Choose a Regularization Parameter $\\lambda$\n",
    "\n",
    "For Ridge regression, we need to select a regularization parameter $\\lambda$ that controls the amount of shrinkage applied to the coefficients. Let's assume $\\lambda = 1$ for this example.\n",
    "\n",
    "### Step 4: Construct the Identity Matrix $I$\n",
    "\n",
    "Create an identity matrix $I$ that matches the number of features including the intercept. Since the intercept is not regularized, its corresponding value in $I$ is set to 0.\n",
    "\n",
    "$I = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $\n",
    "\n",
    "### Step 5: Calculate $X^T X + \\lambda I$\n",
    "\n",
    "Compute the matrix multiplication of $X^T$ (the transpose of $X$) and $X$, and then add $\\lambda$ times the identity matrix $I$.\n",
    "\n",
    "$X^T X + \\lambda I = \\begin{bmatrix} 2 & 5 & 7 \\\\ 5 & 17 & 22 \\\\ 7 & 22 & 29 \\end{bmatrix} + \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 5 & 7 \\\\ 5 & 18 & 22 \\\\ 7 & 22 & 30 \\end{bmatrix} $\n",
    "\n",
    "### Step 6: Compute the Inverse\n",
    "\n",
    "Find the inverse of the matrix $X^T X + \\lambda I$.\n",
    "\n",
    "$(X^T X + \\lambda I)^{-1} $\n",
    "\n",
    "### Step 7: Calculate $X^T y$\n",
    "\n",
    "Multiply the transpose of $X$ by the target vector $y$.\n",
    "\n",
    "$X^T y = \\begin{bmatrix} 9 \\\\ 33 \\\\ 42 \\end{bmatrix} $\n",
    "\n",
    "### Step 8: Solve for $\\hat{\\theta}$\n",
    "\n",
    "Multiply the inverse from Step 6 by the result from Step 7 to get the estimated coefficients $\\hat{\\theta}$.\n",
    "\n",
    "$\\hat{\\theta} = (X^T X + \\lambda I)^{-1} X^T y $\n",
    "\n",
    "Let's carry out these computations to find the Ridge regression coefficients $\\hat{\\theta}$.\n",
    "\n",
    "Using the Ridge regression closed-form formula on the given dataset, the estimated coefficients $\\hat{\\theta}$ are calculated as:\n",
    "\n",
    "$\\hat{\\theta} = \\begin{bmatrix} 1.8 \\\\ 0.45 \\\\ 0.45 \\end{bmatrix} $\n",
    "\n",
    "This vector $\\hat{\\theta}$ includes the estimated values for the intercept ($\\theta_0 = 1.8$) and the coefficients for the features $x_1$ ($\\theta_1 = 0.45$) and $x_2$ ($\\theta_2 = 0.45$), after applying Ridge regularization with $\\lambda = 1$. The regularization term has effectively shrunk the coefficients towards zero compared to what might have been obtained using ordinary least squares regression, thereby reducing the risk of overfitting and potentially improving the model's generalization performance.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1a161d3363212c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
