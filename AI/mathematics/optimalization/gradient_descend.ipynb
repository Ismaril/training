{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GRADIENT DESCENT\n",
    "\n",
    "The gradient vector of a cost function in the context of linear regression is a vector that contains all the partial derivatives of the cost function with respect to each parameter in the model. For a model with parameters $\\theta_0$ (intercept), $\\theta_1$ (coefficient for $x_1$), and $\\theta_2$ (coefficient for $x_2$), the gradient vector can be represented as:\n",
    "\n",
    "$\\nabla_{\\theta} J(\\theta) = \\left[ \\frac{\\partial J(\\theta)}{\\partial \\theta_0}, \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, \\frac{\\partial J(\\theta)}{\\partial \\theta_2} \\right]$\n",
    "\n",
    "### Mean Squared Error (MSE) Cost Function:\n",
    "\n",
    "The MSE cost function for linear regression is given by:\n",
    "\n",
    "$J(\\theta) = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$\n",
    "\n",
    "Where $\\hat{y}_i = \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2}$ is the predicted value.\n",
    "\n",
    "### Partial Derivatives:\n",
    "\n",
    "The partial derivatives of the MSE cost function with respect to each parameter are:\n",
    "\n",
    "1. $\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \\cdot 1$\n",
    "2. $\\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \\cdot x_{i1}$\n",
    "3. $\\frac{\\partial J(\\theta)}{\\partial \\theta_2} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \\cdot x_{i2}$\n",
    "\n",
    "### Given Dataset:\n",
    "\n",
    "Assuming a dataset with two features ($x_1$ and $x_2$) and two data points:\n",
    "\n",
    "| $x_1$ | $x_2$   | $y$   |\n",
    "|-------|---------|-------|\n",
    "| 1     | 2       | 3     |\n",
    "| 4     | 5       | 6     |\n",
    "\n",
    "### Model Parameters:\n",
    "\n",
    "Assuming initial model parameters:\n",
    "\n",
    "- $\\theta_0 = 0.5$\n",
    "- $\\theta_1 = 1$\n",
    "- $\\theta_2 = 0.5$\n",
    "\n",
    "### Computation:\n",
    "\n",
    "We will compute the gradient vector $\\nabla_{\\theta} J(\\theta)$ using the given data and parameters.\n",
    "\n",
    "1. Compute $\\hat{y}_i$ for each observation.\n",
    "2. Compute the partial derivatives $\\frac{\\partial J(\\theta)}{\\partial \\theta_0}$, $\\frac{\\partial J(\\theta)}{\\partial \\theta_1}$, and $\\frac{\\partial J(\\theta)}{\\partial \\theta_2}$.\n",
    "3. Combine the partial derivatives to form the gradient vector $\\nabla_{\\theta} J(\\theta)$.\n",
    "\n",
    "Let's perform these computations.\n",
    "\n",
    "The computed gradient vector $\\nabla_{\\theta} J(\\theta)$ of the Mean Squared Error (MSE) cost function with respect to all parameters ($\\theta_0, \\theta_1, \\theta_2$) for the given dataset and model parameters is:\n",
    "\n",
    "$\\nabla_{\\theta} J(\\theta) = \\begin{bmatrix} 0.5 \\\\ 3.5 \\\\ 4.0 \\end{bmatrix}$\n",
    "\n",
    "This gradient vector contains the partial derivatives of the MSE cost function with respect to each parameter in the model:\n",
    "- The first element (0.5) is the partial derivative with respect to $\\theta_0$ (the intercept term).\n",
    "- The second element (3.5) is the partial derivative with respect to $\\theta_1$ (the coefficient of feature $x_1$).\n",
    "- The third element (4.0) is the partial derivative with respect to $\\theta_2$ (the coefficient of feature $x_2$).\n",
    "\n",
    "These values indicate the direction and magnitude of the steepest ascent in the cost function space. In gradient descent optimization, the parameters $\\theta$ would be updated in the opposite direction of this gradient to minimize the cost function.\n",
    "\n",
    "So in this example, where we have 3 thetas (parameters) and we are moving down to a minimum in a 4 dimensional space because we have 3 parameters as a result, and result of cost function using these parameters with features is the 4th dimension."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "913c01f1422f3de4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you do not want to compute each partial derivative individually, you can use the following formula to compute the gradient vector all at once:\n",
    "\n",
    "$\\nabla_{\\theta} J(\\theta) = \\frac{2}{m} \\cdot X^T \\cdot (X \\cdot \\theta - y)$\n",
    "\n",
    "Where $X$ is the matrix of features, $y$ is the vector of target values, and $\\theta$ is the vector of model parameters.\n",
    "\n",
    "This means that in this particular example:\n",
    "$\\nabla_{\\theta} J(\\theta) = \\frac{2}{m} \\cdot X^T \\cdot (X \\cdot \\theta - y)$ = $\\frac{2}{2} \\cdot \\begin{bmatrix} 1 & 1 & 2 \\\\ 1 & 4 & 5 \\end{bmatrix}^T \\cdot \\left( \\begin{bmatrix} 1 & 1 & 2 \\\\ 1 & 4 & 5 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ 1 \\\\ 0.5 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix} \\right)$ = $\\begin{bmatrix} 0.5 \\\\ 3.5 \\\\ 4.0 \\end{bmatrix}$\n",
    "\n",
    "This means that also in this particular example:\n",
    "$\\nabla_{\\theta} J(\\theta) = \\left[ \\frac{\\partial J(\\theta)}{\\partial \\theta_0}, \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, \\frac{\\partial J(\\theta)}{\\partial \\theta_2} \\right]$ = $\\frac{2}{m} \\cdot X^T \\cdot (X \\cdot \\theta - y)$\n",
    "\n",
    "And in general:\n",
    "$\\nabla_{\\theta} J(\\theta) = \\left[ \\frac{\\partial J(\\theta)}{\\partial \\theta_0}, \\frac{\\partial J(\\theta)}{\\partial \\theta_1}, \\frac{\\partial J(\\theta)}{\\partial \\theta_2}, \\dots, \\frac{\\partial J(\\theta)}{\\partial \\theta_n} \\right]$ = $\\frac{2}{m} \\cdot X^T \\cdot (X \\cdot \\theta - y)$\n",
    "\n",
    "Where $m$ is the number of observations in the dataset, $X$ is the matrix of features, $y$ is the vector of target values, and $\\theta$ is the vector of model parameters.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54041948bbe6876c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(2.5, 7.0)"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0_pred = 0.5*1 + 1*1 + 0.5*2\n",
    "y1_pred = 0.5*1 + 1*4 + 0.5*5\n",
    "y0_true = 3\n",
    "y1_true = 6\n",
    "y0_pred, y1_pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T17:26:30.316276900Z",
     "start_time": "2024-02-05T17:26:30.271071Z"
    }
   },
   "id": "f7f3c72ee33ba16d",
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(-0.5, -3.5, -4.0)"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partial derivatives for each parameter based on the formulas provided above in markdown text.\n",
    "theta_0 = 2/2 * (y0_true - y0_pred)*1 + (y1_true - y1_pred)*1\n",
    "theta_1 = 2/2 * (y0_true - y0_pred)*1 + (y1_true - y1_pred)*4\n",
    "theta_2 = 2/2 * (y0_true - y0_pred)*2 + (y1_true - y1_pred)*5\n",
    "\n",
    "theta_0, theta_1, theta_2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T17:26:30.353412300Z",
     "start_time": "2024-02-05T17:26:30.317274900Z"
    }
   },
   "id": "5ac5ff679f80c521",
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "749e26ec29c5593b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.5, 3.5, 4. ])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Corrected feature matrix X (including a column of ones for the intercept term, which has to be there added manually)\n",
    "X = np.array([[1, 1, 2],\n",
    "              [1, 4, 5]])\n",
    "\n",
    "# Theta vector (assuming the first element is the intercept term)\n",
    "thetas = np.array([0.5, 1, 0.5])\n",
    "\n",
    "# Target vector y remains the same\n",
    "y = np.array([3,\n",
    "              6])\n",
    "\n",
    "# Computing the gradient vector using closed form solution\n",
    "gradient_vector = (2 / 2) * X.T.dot(X.dot(thetas) - y)\n",
    "\n",
    "gradient_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T17:26:30.384395900Z",
     "start_time": "2024-02-05T17:26:30.355380700Z"
    }
   },
   "id": "315b630daaf4b8b7",
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.25, 1.75, 2.  ])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing the gradient vector (provided you used slightly different cost formula)\n",
    "gradient_vector = (1 / 2) * X.T.dot(X.dot(thetas) - y)\n",
    "gradient_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T17:26:30.403567300Z",
     "start_time": "2024-02-05T17:26:30.386367800Z"
    }
   },
   "id": "1b2f2c2d6239ad3a",
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Descent Step:\n",
    "\n",
    "The gradient descent step is given by:\n",
    "\n",
    "$\\theta = \\theta - \\alpha \\cdot \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "Where\n",
    "$\\alpha$ - is the learning rate.\n",
    "$\\theta$ - is the vector of model parameters.\n",
    "$\\nabla_{\\theta} J(\\theta)$ - is the gradient vector of the cost function with respect to the model parameters.\n",
    "\n",
    "### Computation:\n",
    "This is the computation of individual parameters separately:\n",
    "$\\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_0}$\n",
    "$\\theta_1 = \\theta_1 - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_1}$\n",
    "$\\theta_2 = \\theta_2 - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_2}$\n",
    "\n",
    "$\\theta_0 = 0.5 - 0.001 \\cdot 0.5$ = $0.4995$\n",
    "$\\theta_1 = 1 - 0.001 \\cdot 3.5$ = $0.9965$\n",
    "$\\theta_2 = 0.5 - 0.001 \\cdot 4.0$ = $0.496$\n",
    "\n",
    "This is the computation of all parameters at once:\n",
    "$\\theta = \\theta - \\alpha \\cdot \\nabla_{\\theta} J(\\theta)$\n",
    "$\\theta = \\begin{bmatrix} 0.5 \\\\ 1 \\\\ 0.5 \\end{bmatrix} - 0.001 \\cdot \\begin{bmatrix} 0.5 \\\\ 3.5 \\\\ 4.0 \\end{bmatrix}$ = $\\begin{bmatrix} 0.4995 \\\\ 0.9965 \\\\ 0.496 \\end{bmatrix}$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd1c97b5a45c54e4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old thetas:\n",
      "[0.5 1.  0.5]\n",
      "New thetas:\n",
      "[0.49975 0.99825 0.498  ]\n"
     ]
    }
   ],
   "source": [
    "theta_new = thetas - np.dot(0.001, gradient_vector)\n",
    "print(f\"Old thetas:\\n{thetas}\")\n",
    "print(f\"New thetas:\\n{theta_new}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T17:26:30.456973400Z",
     "start_time": "2024-02-05T17:26:30.405563500Z"
    }
   },
   "id": "d0095e9f8d697656",
   "execution_count": 85
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
