{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# LASSO REGRESSION\n",
    "\n",
    "Lasso regression (Least Absolute Shrinkage and Selection Operator) extends linear regression by adding a regularization term that involves the absolute values of the coefficients ($\\theta$ parameters). This can lead to some coefficients being exactly zero, which is why Lasso can also be used for feature selection. The cost function for Lasso regression is given by:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 + \\lambda \\sum_{j=1}^{m} |\\theta_j|$\n",
    "\n",
    "where:\n",
    "- $n$ is the number of observations.\n",
    "- $\\hat{y}_i$ is the predicted value for the \\(i\\)-th observation, calculated as $\\hat{y}_i = \\theta_0 + \\theta_1 x_{i1} + \\theta_2 x_{i2} ...$ in case of linear regression.\n",
    "- $y_i$ is the actual value for the \\(i\\)-th observation.\n",
    "- $\\lambda$  is the regularization parameter, a non-negative hyperparameter that controls the strength of the regularization. A larger $\\lambda$ results in more significant regularization.\n",
    "- $\\theta_j$ is the $j-th$ coefficient or parameter of the model, corresponding to the $j-th$ feature. This includes all coefficients except the intercept/bias term. (Generally, the intercept is not regularized.) \n",
    "- $|\\theta_j|$ denotes the absolute value of $\\theta_j$, which ensures that the regularization term is always positive or zero, regardless of whether $\\theta_j$ is positive or negative.\n",
    "\n",
    "Unlike Ridge regression, Lasso regression does not have a closed-form solution (closed form = computing the parameters directly, without iterative approach) due to the absolute value in the regularization term, and it is typically solved using numerical optimization techniques such as coordinate descent.\n",
    "\n",
    "### Step-by-Step Explanation Using the Given Dataset:\n",
    "\n",
    "Given the dataset:\n",
    "\n",
    "| $x_1$   | $x_2$   | $y$   |\n",
    "|---------|---------|-------|\n",
    "| 1       | 2       | 3     |\n",
    "| 4       | 5       | 6     |\n",
    "\n",
    "#### Step 1: Feature Matrix $X$ and Target Vector $y$\n",
    "\n",
    "First, we construct the feature matrix $X$ by including a column of ones for the intercept and then columns for each feature:\n",
    "\n",
    "$X = \\begin{bmatrix} 1 & 1 & 2 \\\\ 1 & 4 & 5 \\end{bmatrix}, \\quad y = \\begin{bmatrix} 3 \\\\ 6 \\end{bmatrix}$\n",
    "\n",
    "#### Step 2: Initialization\n",
    "\n",
    "We initialize the coefficients $\\theta$ (including $\\theta_0$ for the intercept) to some starting values, e.g., all zeros or small random numbers.\n",
    "\n",
    "#### Step 3: Numerical Optimization\n",
    "\n",
    "Using an optimization algorithm like coordinate descent, we iteratively update each $\\theta_j$ to minimize the cost function $J(\\theta)$. Each update involves solving:\n",
    "\n",
    "$\\theta_j = \\text{argmin}_{\\theta_j} \\left( \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 + \\lambda |\\theta_j| \\right)$\n",
    "\n",
    "where $\\hat{y}_i$ is updated based on the current estimates of $\\theta$.\n",
    "\n",
    "#### Step 4: Convergence Check\n",
    "\n",
    "The algorithm iterates until the changes in the cost function or the coefficients are below a certain threshold, indicating convergence.\n",
    "\n",
    "#### Practical Consideration\n",
    "\n",
    "Since Lasso regression involves an absolute value term in its regularization, it's not differentiable at zero, complicating the optimization. Techniques like coordinate descent handle this by updating one coefficient at a time while holding the others fixed, effectively turning the problem into a series of one-dimensional optimizations that can be solved analytically for each coefficient.\n",
    "\n",
    "Due to the complexity and iterative nature of solving Lasso regression, demonstrating the exact numerical steps with the given dataset would require implementing the optimization algorithm, which is beyond a simple explanation but hopefully, this overview provides insight into how Lasso regression works conceptually."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3d801808cb66234"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
