{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# INTRODUCTION TO ARTIFICIAL NEURAL NETWORKS WITH KERAS\n",
    "Birds inspired us to fly, burdock plants inspired Velcro, and nature has inspired countless more inventions. It seems only logical, then, to look at the brain’s architecture for inspiration on how to build an intelligent machine. This is the logic that sparked artificial neural networks (ANNs), machine learning models inspired by the networks of biological neurons found in our brains. However, although planes were inspired by birds, they don’t have to flap their wings to fly. Similarly, ANNs have gradually become quite different from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying “units” rather than “neurons”), lest we restrict our creativity to biologically plausible systems.⁠1\n",
    "\n",
    "ANNs are at the very core of deep learning. They are versatile, powerful, and scalable, making them ideal to tackle large and highly complex machine learning tasks such as classifying billions of images (e.g., Google Images), powering speech recognition services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds of millions of users every day (e.g., YouTube), or learning to beat the world champion at the game of Go (DeepMind’s AlphaGo).\n",
    "\n",
    "The first part of this chapter introduces artificial neural networks, starting with a quick tour of the very first ANN architectures and leading up to multilayer perceptrons, which are heavily used today (other architectures will be explored in the next chapters). In the second part, we will look at how to implement neural networks using TensorFlow’s Keras API. This is a beautifully designed and simple high-level API for building, training, evaluating, and running neural networks. But don’t be fooled by its simplicity: it is expressive and flexible enough to let you build a wide variety of neural network architectures. In fact, it will probably be sufficient for most of your use cases. And should you ever need extra flexibility, you can always write custom Keras components using its lower-level API, or even use TensorFlow directly, as you will see in Chapter 12.\n",
    "\n",
    "But first, let’s go back in time to see how artificial neural networks came to be!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d19bbbd0b06c3dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FROM BIOLOGICAL TO ARTIFICIAL NEURONS\n",
    "Surprisingly, ANNs have been around for quite a while: they were first introduced back in 1943 by the neurophysiologist Warren McCulloch and the mathematician Walter Pitts. In their landmark paper⁠2 “A Logical Calculus of Ideas Immanent in Nervous Activity”, McCulloch and Pitts presented a simplified computational model of how biological neurons might work together in animal brains to perform complex computations using propositional logic. This was the first artificial neural network architecture. Since then many other architectures have been invented, as you will see.\n",
    "\n",
    "The early successes of ANNs led to the widespread belief that we would soon be conversing with truly intelligent machines. When it became clear in the 1960s that this promise would go unfulfilled (at least for quite a while), funding flew elsewhere, and ANNs entered a long winter. In the early 1980s, new architectures were invented and better training techniques were developed, sparking a revival of interest in connectionism, the study of neural networks. But progress was slow, and by the 1990s other powerful machine learning techniques had been invented, such as support vector machines (see Chapter 5). These techniques seemed to offer better results and stronger theoretical foundations than ANNs, so once again the study of neural networks was put on hold.\n",
    "\n",
    "We are now witnessing yet another wave of interest in ANNs. Will this wave die out like the previous ones did? Well, here are a few good reasons to believe that this time is different and that the renewed interest in ANNs will have a much more profound impact on our lives:\n",
    "\n",
    "- There is now a huge quantity of data available to train neural networks, and ANNs frequently outperform other ML techniques on very large and complex problems.\n",
    "\n",
    "- The tremendous increase in computing power since the 1990s now makes it possible to train large neural networks in a reasonable amount of time. This is in part due to Moore’s law (the number of components in integrated circuits has doubled about every 2 years over the last 50 years), but also thanks to the gaming industry, which has stimulated the production of powerful GPU cards by the millions. Moreover, cloud platforms have made this power accessible to everyone.\n",
    "\n",
    "- The training algorithms have been improved. To be fair they are only slightly different from the ones used in the 1990s, but these relatively small tweaks have had a huge positive impact.\n",
    "\n",
    "- Some theoretical limitations of ANNs have turned out to be benign in practice. For example, many people thought that ANN training algorithms were doomed because they were likely to get stuck in local optima, but it turns out that this is not a big problem in practice, especially for larger neural networks: the local optima often perform almost as well as the global optimum.\n",
    "\n",
    "- ANNs seem to have entered a virtuous circle of funding and progress. Amazing products based on ANNs regularly make the headline news, which pulls more and more attention and funding toward them, resulting in more and more progress and even more amazing products."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6488d688f96961d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BIOLOGICAL NEURONS\n",
    "Before we discuss artificial neurons, let’s take a quick look at a biological neuron (represented in Figure 10-1). It is an unusual-looking cell mostly found in animal brains. It’s composed of a cell body containing the nucleus and most of the cell’s complex components, many branching extensions called dendrites, plus one very long extension called the axon. The axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called telodendria, and at the tip of these branches are minuscule structures called synaptic terminals (or simply synapses), which are connected to the dendrites or cell bodies of other neurons.⁠3 Biological neurons produce short electrical impulses called action potentials (APs, or just signals), which travel along the axons and make the synapses release chemical signals called neurotransmitters. When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses (actually, it depends on the neurotransmitters, as some of them inhibit the neuron from firing)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "828ae9164a1bd17e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-1. A biological neuron\n",
    "!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f74f6f20a3689f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thus, individual biological neurons seem to behave in a simple way, but they’re organized in a vast network of billions, with each neuron typically connected to thousands of other neurons. Highly complex computations can be performed by a network of fairly simple neurons, much like a complex anthill can emerge from the combined efforts of simple ants. The architecture of biological neural networks (BNNs)⁠5 is the subject of active research, but some parts of the brain have been mapped. These efforts show that neurons are often organized in consecutive layers, especially in the cerebral cortex (the outer layer of the brain), as shown in Figure 10-2."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2654ab49b4a777b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-2. Multiple layers in a biological neural network (human cortex)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51387b05db0c829"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOGICAL COMPUTATIONS WITH NEURONS\n",
    "McCulloch and Pitts proposed a very simple model of the biological neuron, which later became known as an artificial neuron: it has one or more binary (on/off) inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active. In their paper, McCulloch and Pitts showed that even with such a simplified model it is possible to build a network of artificial neurons that can compute any logical proposition you want. To see how such a network works, let’s build a few ANNs that perform various logical computations (see Figure 10-3), assuming that a neuron is activated when at least two of its input connections are active."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ea4e09aa55d6b76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-3. ANNs performing simple logical computations\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af147f99663cfc79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s see what these networks do:\n",
    "\n",
    "- The first network on the left is the identity function: if neuron A is activated, then neuron C gets activated as well (since it receives two input signals from neuron A); but if neuron A is off, then neuron C is off as well.\n",
    "\n",
    "- The second network performs a logical AND: neuron C is activated only when both neurons A and B are activated (a single input signal is not enough to activate neuron C).\n",
    "\n",
    "- The third network performs a logical OR: neuron C gets activated if either neuron A or neuron B is activated (or both).\n",
    "\n",
    "- Finally, if we suppose that an input connection can inhibit the neuron’s activity (which is the case with biological neurons), then the fourth network computes a slightly more complex logical proposition: neuron C is activated only if neuron A is active and neuron B is off. If neuron A is active all the time, then you get a logical NOT: neuron C is active when neuron B is off, and vice versa.\n",
    "\n",
    "You can imagine how these networks can be combined to compute complex logical expressions (see the exercises at the end of the chapter for an example)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d9af07794b6bbe1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE PERCEPTRON\n",
    "The perceptron is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4) called a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU). The inputs and output are numbers (instead of binary on/off values), and each input connection is associated with a weight. The TLU first computes a linear function of its inputs: z = w1 x1 + w2 x2 + ⋯ + wn xn + b = w⊺ x + b. Then it applies a step function to the result: hw(x) = step(z). So it’s almost like logistic regression, except it uses a step function instead of the logistic function (Chapter 4). Just like in logistic regression, the model parameters are the input weights w and the bias term b."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7939253d312a92c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-4. TLU: an artificial neuron that computes a weighted sum of its inputs w⊺ x, plus a bias term b, then applies a step function\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75e336f8fd395ed8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most common step function used in perceptrons is the Heaviside step function (see Equation 10-1). Sometimes the sign function is used instead."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cec8db375a60b0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Equation 10-1. Common step functions used in perceptrons (assuming threshold = 0)\n",
    "heaviside$(z) = \\begin{cases} 0 & \\text{if } z < 0 \\\\ 1 & \\text{if } z \\geq 0 \\end{cases}$\\\n",
    "$ sign(z) = \\begin{cases} -1 & \\text{if } z < 0 \\\\ 0 & \\text{if } z = 0 \\\\ 1 & \\text{if } z > 0 \\end{cases}$\n",
    "\n",
    "A single TLU can be used for simple linear binary classification. It computes a linear function of its inputs, and if the result exceeds a threshold, it outputs the positive class. Otherwise, it outputs the negative class. This may remind you of logistic regression (Chapter 4) or linear SVM classification (Chapter 5). You could, for example, use a single TLU to classify iris flowers based on petal length and width. Training such a TLU would require finding the right values for w1, w2, and b (the training algorithm is discussed shortly).\n",
    "\n",
    "A perceptron is composed of one or more TLUs organized in a single layer, where every TLU is connected to every input. Such a layer is called a fully connected layer, or a dense layer. The inputs constitute the input layer. And since the layer of TLUs produces the final outputs, it is called the output layer. For example, a perceptron with two inputs and three outputs is represented in Figure 10-5."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d30dad749c43591c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-5. Architecture of a perceptron with two inputs and three output neurons\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f311d13e27c1567"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This perceptron can classify instances simultaneously into three different binary classes, which makes it a multilabel classifier. It may also be used for multiclass classification.\n",
    "\n",
    "Thanks to the magic of linear algebra, Equation 10-2 can be used to efficiently compute the outputs of a layer of artificial neurons for several instances at once."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a07a742d3147a8fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Equation 10-2. Computing the outputs of a fully connected layer\n",
    "$h_{W,b}(X) = \\phi(XW + b)$\n",
    "\n",
    "In this equation:\n",
    "- As always, X represents the matrix of input features. It has one row per instance and one column per feature.\n",
    "\n",
    "- The weight matrix W contains all the connection weights. It has one row per input and one column per neuron.\n",
    "\n",
    "- The bias vector b contains all the bias terms: one per neuron.\n",
    "\n",
    "- The function ϕ is called the activation function: when the artificial neurons are TLUs, it is a step function (we will discuss other activation functions shortly).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5d91f4b382be9c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "In mathematics, the sum of a matrix and a vector is undefined. However, in data science, we allow “broadcasting”: adding a vector to a matrix means adding it to every row in the matrix. So, XW + b first multiplies X by W—which results in a matrix with one row per instance and one column per output—then adds the vector b to every row of that matrix, which adds each bias term to the corresponding output, for every instance. Moreover, ϕ is then applied itemwise to each item in the resulting matrix."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7a4dccb413e4366"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, how is a perceptron trained? The perceptron training algorithm proposed by Rosenblatt was largely inspired by Hebb’s rule. In his 1949 book The Organization of Behavior (Wiley), Donald Hebb suggested that when a biological neuron triggers another neuron often, the connection between these two neurons grows stronger. Siegrid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire together, wire together”; that is, the connection weight between two neurons tends to increase when they fire simultaneously. This rule later became known as Hebb’s rule (or Hebbian learning). Perceptrons are trained using a variant of this rule that takes into account the error made by the network when it makes a prediction; the perceptron learning rule reinforces connections that help reduce the error. More specifically, the perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. The rule is shown in Equation 10-3."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bec0ca7ceac5083d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Equation 10-3. Perceptron learning rule (weight update)\n",
    "$w_{i,j}^{(next step)} = w_{i,j} + \\eta(y_j - \\hat{y}_j)x_i$\n",
    "\n",
    "In this equation:\n",
    "- wi, j is the connection weight between the ith input and the jth neuron.\n",
    "\n",
    "- xi is the ith input value of the current training instance.\n",
    "\n",
    "- $\\hat{y_j}$ is the output of the jth output neuron for the current training instance.\n",
    "\n",
    "- yj is the target output of the jth output neuron for the current training instance.\n",
    "\n",
    "- η is the learning rate (see Chapter 4).\n",
    "\n",
    "The decision boundary of each output neuron is linear, so perceptrons are incapable of learning complex patterns (just like logistic regression classifiers). However, if the training instances are linearly separable, Rosenblatt demonstrated that this algorithm would converge to a solution.⁠7 This is called the perceptron convergence theorem.\n",
    "\n",
    "Scikit-Learn provides a Perceptron class that can be used pretty much as you would expect—for example, on the iris dataset (introduced in Chapter 4):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80a36200b88f0199"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y = (iris.target == 0)  # Iris setosa\n",
    "\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "X_new = [[2, 0.5], [3, 1]]\n",
    "y_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b644638ede927ec2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may have noticed that the perceptron learning algorithm strongly resembles stochastic gradient descent (introduced in Chapter 4). In fact, Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization).\n",
    "\n",
    "In their 1969 monograph Perceptrons, Marvin Minsky and Seymour Papert highlighted a number of serious weaknesses of perceptrons—in particular, the fact that they are incapable of solving some trivial problems (e.g., the exclusive OR (XOR) classification problem; see the left side of Figure 10-6). This is true of any other linear classification model (such as logistic regression classifiers), but researchers had expected much more from perceptrons, and some were so disappointed that they dropped neural networks altogether in favor of higher-level problems such as logic, problem solving, and search. The lack of practical applications also didn’t help.\n",
    "\n",
    "It turns out that some of the limitations of perceptrons can be eliminated by stacking multiple perceptrons. The resulting ANN is called a multilayer perceptron (MLP). An MLP can solve the XOR problem, as you can verify by computing the output of the MLP represented on the right side of Figure 10-6: with inputs (0, 0) or (1, 1), the network outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. Try verifying that this network indeed solves the XOR problem!8"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af84bf4b31369c22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-6. XOR classification problem and an MLP that solves it\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8f7edd0c6496ff2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "Contrary to logistic regression classifiers, perceptrons do not output a class probability. This is one reason to prefer logistic regression over perceptrons. Moreover, perceptrons do not use any regularization by default, and training stops as soon as there are no more prediction errors on the training set, so the model typically does not generalize as well as logistic regression or a linear SVM classifier. However, perceptrons may train a bit faster."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1f29a293df3527b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE MULTILAYER PERCEPTRON AND BACKPROPAGATION\n",
    "An MLP is composed of one input layer, one or more layers of TLUs called hidden layers, and one final layer of TLUs called the output layer (see Figure 10-7). The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e78f09be8e00f56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-7. Architecture of a multilayer perceptron with two inputs, one hidden layer of four neurons, and three output neurons\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1666eb102cb7469d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "The signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a feedforward neural network (FNN)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19b52a7c747251e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When an ANN contains a deep stack of hidden layers,⁠9 it is called a deep neural network (DNN). The field of deep learning studies DNNs, and more generally it is interested in models containing deep stacks of computations. Even so, many people talk about deep learning whenever neural networks are involved (even shallow ones).\n",
    "\n",
    "For many years researchers struggled to find a way to train MLPs, without success. In the early 1960s several researchers discussed the possibility of using gradient descent to train neural networks, but as we saw in Chapter 4, this requires computing the gradients of the model’s error with regard to the model parameters; it wasn’t clear at the time how to do this efficiently with such a complex model containing so many parameters, especially with the computers they had back then.\n",
    "\n",
    "Then, in 1970, a researcher named Seppo Linnainmaa introduced in his master’s thesis a technique to compute all the gradients automatically and efficiently. This algorithm is now called reverse-mode automatic differentiation (or reverse-mode autodiff for short). In just two passes through the network (one forward, one backward), it is able to compute the gradients of the neural network’s error with regard to every single model parameter. In other words, it can find out how each connection weight and each bias should be tweaked in order to reduce the neural network’s error. These gradients can then be used to perform a gradient descent step. If you repeat this process of computing the gradients automatically and taking a gradient descent step, the neural network’s error will gradually drop until it eventually reaches a minimum. This combination of reverse-mode autodiff and gradient descent is now called backpropagation (or backprop for short)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46c16f7389c4ff1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "There are various autodiff techniques, with different pros and cons. Reverse-mode autodiff is well suited when the function to differentiate has many variables (e.g., connection weights and biases) and few outputs (e.g., one loss). If you want to learn more about autodiff, check out Appendix B."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a54a779ad7a15ecf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Backpropagation can actually be applied to all sorts of computational graphs, not just neural networks: indeed, Linnainmaa’s master’s thesis was not about neural nets, it was more general. It was several more years before backprop started to be used to train neural networks, but it still wasn’t mainstream. Then, in 1985, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a groundbreaking paper⁠10 analyzing how backpropagation allowed neural networks to learn useful internal representations. Their results were so impressive that backpropagation was quickly popularized in the field. Today, it is by far the most popular training technique for neural networks.\n",
    "\n",
    "Let’s run through how backpropagation works again in a bit more detail:\n",
    "- It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an epoch.\n",
    "\n",
    "- Each mini-batch enters the network through the input layer. The algorithm then computes the output of all the neurons in the first hidden layer, for every instance in the mini-batch. The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the forward pass: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "- Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "- Then it computes how much each output bias and each connection to the output layer contributed to the error. This is done analytically by applying the chain rule (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until it reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights and biases in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "- Finally, the algorithm performs a gradient descent step to tweak all the connection weights in the network, using the error gradients it just computed."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f79c778292aff74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "WARNING\n",
    "It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won’t be too smart. If instead you randomly initialize the weights, you break the symmetry and allow backpropagation to train a diverse team of neurons."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d062eb921a84e852"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In short, backpropagation makes predictions for a mini-batch (forward pass), measures the error, then goes through each layer in reverse to measure the error contribution from each parameter (reverse pass), and finally tweaks the connection weights and biases to reduce the error (gradient descent step).\n",
    "\n",
    "In order for backprop to work properly, Rumelhart and his colleagues made a key change to the MLP’s architecture: they replaced the step function with the logistic function, σ(z) = 1 / (1 + exp(–z)), also called the sigmoid function. This was essential because the step function contains only flat segments, so there is no gradient to work with (gradient descent cannot move on a flat surface), while the sigmoid function has a well-defined nonzero derivative everywhere, allowing gradient descent to make some progress at every step. In fact, the backpropagation algorithm works well with many other activation functions, not just the sigmoid function. Here are two other popular choices:\n",
    "\n",
    "**The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1**\n",
    "Just like the sigmoid function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in the case of the sigmoid function). That range tends to make each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "\n",
    "**The rectified linear unit function: ReLU(z) = max(0, z)**\n",
    "The ReLU function is continuous but unfortunately not differentiable at z = 0 (the slope changes abruptly, which can make gradient descent bounce around), and its derivative is 0 for z < 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default.⁠11 Importantly, the fact that it does not have a maximum output value helps reduce some issues during gradient descent (we will come back to this in Chapter 11).\n",
    "\n",
    "These popular activation functions and their derivatives are represented in Figure 10-8. But wait! Why do we need activation functions in the first place? Well, if you chain several linear transformations, all you get is a linear transformation. For example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can’t solve very complex problems with that. Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26cd8d97683c8468"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Figure 10-8. Activation functions (left) and their derivatives (right)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "618d6a969817c3dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "OK! You know where neural nets came from, what their architecture is, and how to compute their outputs. You’ve also learned about the backpropagation algorithm. But what exactly can you do with neural nets?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "680ed6095bd4b8bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# REGRESSION MLPs\n",
    "First, MLPs can be used for regression tasks. If you want to predict a single value (e.g., the price of a house, given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimension. For example, to locate the center of an object in an image, you need to predict 2D coordinates, so you need two output neurons. If you also want to place a bounding box around the object, then you need two more numbers: the width and the height of the object. So, you end up with four output neurons.\n",
    "\n",
    "Scikit-Learn includes an MLPRegressor class, so let’s use it to build an MLP with three hidden layers composed of 50 neurons each, and train it on the California housing dataset. For simplicity, we will use Scikit-Learn’s fetch_california_housing() function to load the data. This dataset is simpler than the one we used in Chapter 2, since it contains only numerical features (there is no ocean_proximity feature), and there are no missing values. The following code starts by fetching and splitting the dataset, then it creates a pipeline to standardize the input features before sending them to the MLPRegressor. This is very important for neural networks because they are trained using gradient descent, and as we saw in Chapter 4, gradient descent does not converge very well when the features have very different scales. Finally, the code trains the model and evaluates its validation error. The model uses the ReLU activation function in the hidden layers, and it uses a variant of gradient descent called Adam (see Chapter 11) to minimize the mean squared error, with a little bit of ℓ2 regularization (which you can control via the alpha hyperparameter):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db63973da974db84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_valid)\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)  # about 0.505"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38c67147b635440b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get a validation RMSE of about 0.505, which is comparable to what you would get with a random forest classifier. Not too bad for a first try!\n",
    "\n",
    "Note that this MLP does not use any activation function for the output layer, so it’s free to output any value it wants. This is generally fine, but if you want to guarantee that the output will always be positive, then you should use the ReLU activation function in the output layer, or the softplus activation function, which is a smooth variant of ReLU: softplus(z) = log(1 + exp(z)). Softplus is close to 0 when z is negative, and close to z when z is positive. Finally, if you want to guarantee that the predictions will always fall within a given range of values, then you should use the sigmoid function or the hyperbolic tangent, and scale the targets to the appropriate range: 0 to 1 for sigmoid and –1 to 1 for tanh. Sadly, the MLPRegressor class does not support activation functions in the output layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1abf83dcf2ab2cf9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "WARNING\n",
    "Building and training a standard MLP with Scikit-Learn in just a few lines of code is very convenient, but the neural net features are limited. This is why we will switch to Keras in the second part of this chapter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ba0d02025453ac7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MLPRegressor class uses the mean squared error, which is usually what you want for regression, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you may want to use the Huber loss, which is a combination of both. It is quadratic when the error is smaller than a threshold δ (typically 1) but linear when the error is larger than δ. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error. However, MLPRegressor only supports the MSE.\n",
    "\n",
    "Table 10-1 summarizes the typical architecture of a regression MLP."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ac96ce92e2df5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Table 10-1. Typical architecture of a regression MLP\n",
    "\n",
    "| Hyperparameter               | Typical value                                             |\n",
    "|------------------------------|-----------------------------------------------------------|\n",
    "| Hidden layers                | Depends on the problem, but typically 1 to 5              |\n",
    "| Neurons per hidden layer     | Depends on the problem, but typically 10 to 100           |\n",
    "| Output neurons               | 1 per dimension                                           |\n",
    "| Hidden activation            | ReLU                                                      |\n",
    "| Output activation            | None, or ReLU/softplus (if positive outputs) or sigmoid/tanh (if bounded outputs) |\n",
    "| Loss function                | MSE or Huber if outliers                                  |\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "434972ad921c6236"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CLASSIFICATION MLPs\n",
    "MLPs can also be used for classification tasks. For a binary classification problem, you just need a single output neuron using the sigmoid activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. The estimated probability of the negative class is equal to one minus that number.\n",
    "\n",
    "MLPs can also easily handle multilabel binary classification tasks (see Chapter 3). For example, you could have an email classification system that predicts whether each incoming email is ham or spam, and simultaneously predicts whether it is an urgent or nonurgent email. In this case, you would need two output neurons, both using the sigmoid activation function: the first would output the probability that the email is spam, and the second would output the probability that it is urgent. More generally, you would dedicate one output neuron for each positive class. Note that the output probabilities do not necessarily add up to 1. This lets the model output any combination of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and perhaps even urgent spam (although that would probably be an error).\n",
    "\n",
    "If each instance can belong only to a single class, out of three or more possible classes (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class, and you should use the softmax activation function for the whole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4) will ensure that all the estimated probabilities are between 0 and 1 and that they add up to 1, since the classes are exclusive. As you saw in Chapter 3, this is called multiclass classification.\n",
    "\n",
    "Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (or x-entropy or log loss for short, see Chapter 4) is generally a good choice."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0c5e792f084b6b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-9. A modern MLP (including ReLU and softmax) for classification\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5db86638d7e3ad20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scikit-Learn has an MLPClassifier class in the sklearn.neural_network package. It is almost identical to the MLPRegressor class, except that it minimizes the cross entropy rather than the MSE. Give it a try now, for example on the iris dataset. It’s almost a linear task, so a single layer with 5 to 10 neurons should suffice (make sure to scale the features).\n",
    "\n",
    "Table 10-2 summarizes the typical architecture of a classification MLP."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6a0947538506454"
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Hyperparameter | Binary classification | Multilabel binary classification | Multiclass classification |\n",
    "| --- | --- |----------------------------------|---------------------------|\n",
    "| # hidden layers | Typically 1 to 5 layers, depending on the task | same as bin clf                     | same as bin clf           |\n",
    "| # output neurons | 1 | 1 per binary label               | 1 per class               |\n",
    "| Output layer activation | Sigmoid | Sigmoid                          | Softmax                   |\n",
    "| Loss function | X-entropy | X-entropy                        | X-entropy                 |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a22b5a631a58ebbc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "Before we go on, I recommend you go through exercise 1 at the end of this chapter. You will play with various neural network architectures and visualize their outputs using the TensorFlow playground. This will be very useful to better understand MLPs, including the effects of all the hyperparameters (number of layers and neurons, activation functions, and more)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d066727f86d76a20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMPLEMENTING MLPS WITH KERAS\n",
    "Keras is TensorFlow’s high-level deep learning API: it allows you to build, train, evaluate, and execute all sorts of neural networks. The original Keras library was developed by François Chollet as part of a research project⁠12 and was released as a standalone open source project in March 2015. It quickly gained popularity, owing to its ease of use, flexibility, and beautiful design."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fad092f85ca8c47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "Keras used to support multiple backends, including TensorFlow, PlaidML, Theano, and Microsoft Cognitive Toolkit (CNTK) (the last two are sadly deprecated), but since version 2.4, Keras is TensorFlow-only. Similarly, TensorFlow used to include multiple high-level APIs, but Keras was officially chosen as its preferred high-level API when TensorFlow 2 came out. Installing TensorFlow will automatically install Keras as well, and Keras will not work without TensorFlow installed. In short, Keras and TensorFlow fell in love and got married. Other popular deep learning libraries include PyTorch by Facebook and JAX by Google.13"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72e96812e5ddbefe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let’s use Keras! We will start by building an MLP for image classification."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13c06beb331cdd7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "Colab runtimes come with recent versions of TensorFlow and Keras preinstalled. However, if you want to install them on your own machine, please see the installation instructions at https://homl.info/install."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec7dd34b136f5d40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BUILDING AN IMAGE CLASSIFIER USING THE SEQUENTIAL API\n",
    "First, we need to load a dataset. We will use Fashion MNIST, which is a drop-in replacement of MNIST (introduced in Chapter 3). It has the exact same format as MNIST (70,000 grayscale images of 28 × 28 pixels each, with 10 classes), but the images represent fashion items rather than handwritten digits, so each class is more diverse, and the problem turns out to be significantly more challenging than MNIST. For example, a simple linear model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f146b83f5b4ace9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# USING KERAS TO LOAD THE DATASET\n",
    "Keras provides some utility functions to fetch and load common datasets, including MNIST, Fashion MNIST, and a few more. Let’s load Fashion MNIST. It’s already shuffled and split into a training set (60,000 images) and a test set (10,000 images), but we’ll hold out the last 5,000 images from the training set for validation:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e4b3901b2f3fef8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "746efb6fb35f2727"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "TensorFlow is usually imported as tf, and the Keras API is available via tf.keras."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3b81fe7bd04300f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one important difference is that every image is represented as a 28 × 28 array rather than a 1D array of size 784. Moreover, the pixel intensities are represented as integers (from 0 to 255) rather than floats (from 0.0 to 255.0). Let’s take a look at the shape and data type of the training set:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80014d18c3a97548"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e44ccedcf7e2225"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train.dtype"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1ef582f6888d609"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 (this also converts them to floats):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78f0fe4e2d69d62a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f0856742d0c9917"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. Easy. For Fashion MNIST, however, we need the list of class names to know what we are dealing with:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba38d9b229bf3b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d02234e6bcb4a03c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For example, the first image in the training set represents an ankle boot:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf9e5d63df4145a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class_names[y_train[0]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c438a6bafccbb81e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-10 shows some samples from the Fashion MNIST dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d17764691ec9fd4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-10. Samples from Fashion MNIST\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d844c9af911c66bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CREATING THE MODEL USING THE SEQUENTIAL API\n",
    "Now let’s build the neural network! Here is a classification MLP with two hidden layers:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3743f4b0fb56c56"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[28, 28]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e423570efd23dd52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s go through this code line by line:\n",
    "\n",
    "- First, set TensorFlow’s random seed to make the results reproducible: the random weights of the hidden layers and the output layer will be the same every time you run the notebook. You could also choose to use the tf.keras.utils.set_random_seed() function, which conveniently sets the random seeds for TensorFlow, Python (random.seed()), and NumPy (np.random.seed()).\n",
    "\n",
    "- The next line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the sequential API.\n",
    "\n",
    "- Next, we build the first layer (an Input layer) and add it to the model. We specify the input shape, which doesn’t include the batch size, only the shape of the instances. Keras needs to know the shape of the inputs so it can determine the shape of the connection weight matrix of the first hidden layer.\n",
    "\n",
    "- Then we add a Flatten layer. Its role is to convert each input image into a 1D array: for example, if it receives a batch of shape [32, 28, 28], it will reshape it to [32, 784]. In other words, if it receives input data X, it computes X.reshape(-1, 784). This layer doesn’t have any parameters; it’s just there to do some simple preprocessing.\n",
    "\n",
    "- Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes Equation 10-2.\n",
    "\n",
    "- Then we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "\n",
    "- Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function because the classes are exclusive.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "132d9c1b92a1bf90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "Specifying activation=\"relu\" is equivalent to specifying activation=tf.keras.activations.relu. Other activation functions are available in the tf.keras.activations package. We will use many of them in this book; see https://keras.io/api/layers/activations for the full list. We will also define our own custom activation functions in Chapter 12."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f6d678b08c5747d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of adding the layers one by one as we just did, it’s often more convenient to pass a list of layers when creating the Sequential model. You can also drop the Input layer and instead specify the input_shape in the first layer:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f2acc3e283ca02e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f31362703c4c193"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model’s summary() method displays all the model’s layers,⁠14 including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters (you will see some non-trainable parameters later in this chapter):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "262dafffab9438a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ab1d2f88b356a90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that Dense layers often have a lot of parameters. For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data. We will come back to this later.\n",
    "\n",
    "Each layer in a model must have a unique name (e.g., \"dense_2\"). You can set the layer names explicitly using the constructor’s name argument, but generally it’s simpler to let Keras name the layers automatically, as we just did. Keras takes the layer’s class name and converts it to snake case (e.g., a layer from the MyCoolLayer class is named \"my_cool_layer\" by default). Keras also ensures that the name is globally unique, even across models, by appending an index if needed, as in \"dense_2\". But why does it bother making the names unique across models? Well, this makes it possible to merge models easily without getting name conflicts."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebd82be083516fc4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "All global state managed by Keras is stored in a Keras session, which you can clear using tf.keras.backend.clear_session(). In particular, this resets the name counters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c316f7030539c1bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can easily get a model’s list of layers using the layers attribute, or use the get_layer() method to access a layer by name:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ef9b325d1eb6e01"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.layers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15b2ce1846269de7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dd25c3ab39179db"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hidden1.name"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a1dd07cbbc406bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.get_layer(hidden1.name) is hidden1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24b151dfb154f43d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the parameters of a layer can be accessed using its get_weights() and set_weights() methods. For a Dense layer, this includes both the connection weights and the bias terms:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9087e4579360115e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4375d9399ef669f4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weights.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d436147fd985742e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "biases"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b359ee7ba41e665c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "biases.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9719b2e32fc63817"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the Dense layer initialized the connection weights randomly (which is needed to break symmetry, as discussed earlier), and the biases were initialized to zeros, which is fine. If you want to use a different initialization method, you can set kernel_initializer (kernel is another name for the matrix of connection weights) or bias_initializer when creating the layer. We’ll discuss initializers further in Chapter 11, and the full list is at https://keras.io/api/layers/initializers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56edbdc52c07ffd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "The shape of the weight matrix depends on the number of inputs, which is why we specified the input_shape when creating the model. If you do not specify the input shape, it’s OK: Keras will simply wait until it knows the input shape before it actually builds the model parameters. This will happen either when you feed it some data (e.g., during training), or when you call its build() method. Until the model parameters are built, you will not be able to do certain things, such as display the model summary or save the model. So, if you know the input shape when creating the model, it is best to specify it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea99f69a8488e44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# COMPILING THE MODEL\n",
    "After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd05417fb2733288"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "743854baaee46958"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "Using loss=\"sparse_categorical_crossentropy\" is the equivalent of using loss=tf.keras.losses.sparse_categorical_​cross⁠entropy. Similarly, using optimizer=\"sgd\" is the equivalent of using optimizer=tf.keras.optimizers.SGD(), and using metrics=[\"accuracy\"] is the equivalent of using metrics=​[tf.keras.metrics.sparse_categorical_accuracy] (when using this loss). We will use many other losses, optimizers, and metrics in this book; for the full lists, see https://keras.io/api/losses, https://keras.io/api/optimizers, and https://keras.io/api/metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51045a46ec0e0d8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code requires explanation. We use the \"sparse_categorical_crossentropy\" loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g., [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead. If we were doing binary classification or multilabel binary classification, then we would use the \"sigmoid\" activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c227cd9331d85b2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "If you want to convert sparse labels (i.e., class indices) to one-hot vector labels, use the tf.keras.utils.to_categorical() function. To go the other way round, use the np.argmax() function with axis=1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c9c98f7b766c37c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Regarding the optimizer, \"sgd\" means that we will train the model using stochastic gradient descent. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus gradient descent). We will discuss more efficient optimizers in Chapter 11. They improve gradient descent, not autodiff."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4424827d04c9d675"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer=tf.keras. ​opti⁠mizers.SGD(learning_rate=__???__) to set the learning rate, rather than optimizer=\"sgd\", which defaults to a learning rate of 0.01."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92ff24856f366d66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, since this is a classifier, it’s useful to measure its accuracy during training and evaluation, which is why we set metrics=[\"accuracy\"]."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e182dbd9d5f4c45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRAINING AND EVALUATING THE MODEL\n",
    "Now the model is ready to be trained. For this we simply need to call its fit() method:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a83703296b75d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f1719e87aea53b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train (or else it would default to just 1, which would definitely not be enough to converge to a good solution). We also pass a validation set (this is optional). Keras will measure the loss and the extra metrics on this set at the end of each epoch, which is very useful to see how well the model really performs. If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set, or there is a bug, such as a data mismatch between the training set and the validation set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daa9c1d114b09018"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "Shape errors are quite common, especially when getting started, so you should familiarize yourself with the error messages: try fitting a model with inputs and/or labels of the wrong shape, and see the errors you get. Similarly, try compiling the model with loss=\"categorical_crossentropy\" instead of loss=\"sparse_​cat⁠egorical_crossentropy\". Or you can remove the Flatten layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32fb2f8f94fb9a4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that’s it! The neural network is trained. At each epoch during training, Keras displays the number of mini-batches processed so far on the left side of the progress bar. The batch size is 32 by default, and since the training set has 55,000 images, the model goes through 1,719 batches per epoch: 1,718 of size 32, and 1 of size 24. After the progress bar, you can see the mean training time per sample, and the loss and accuracy (or any other extra metrics you asked for) on both the training set and the validation set. Notice that the training loss went down, which is a good sign, and the validation accuracy reached 88.94% after 30 epochs. That’s slightly below the training accuracy, so there is a little bit of overfitting going on, but not a huge amount."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40c498c77fe46b80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "Instead of passing a validation set using the validation_data argument, you could set validation_split to the ratio of the training set that you want Keras to use for validation. For example, validation_split=0.1 tells Keras to use the last 10% of the data (before shuffling) for validation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c9e8111900fdab9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the training set was very skewed, with some classes being overrepresented and others underrepresented, it would be useful to set the class_weight argument when calling the fit() method, to give a larger weight to underrepresented classes and a lower weight to overrepresented classes. These weights would be used by Keras when computing the loss. If you need per-instance weights, set the sample_weight argument. If both class_weight and sample_weight are provided, then Keras multiplies them. Per-instance weights could be useful, for example, if some instances were labeled by experts while others were labeled using a crowdsourcing platform: you might want to give more weight to the former. You can also provide sample weights (but not class weights) for the validation set by adding them as a third item in the validation_data tuple.\n",
    "\n",
    "The fit() method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch), and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). If you use this dictionary to create a Pandas DataFrame and call its plot() method, you get the learning curves shown in Figure 10-11:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39605959d148ba2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
    "plt.show()\n",
    "\n",
    "# Figure 10-11. Learning curves: the mean training loss and accuracy measured over each epoch, and the mean validation loss and accuracy measured at the end of each epoch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12ec6e88ca06a77a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see that both the training accuracy and the validation accuracy steadily increase during training, while the training loss and the validation loss decrease. This is good. The validation curves are relatively close to each other at first, but they get further apart over time, which shows that there’s a little bit of overfitting. In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training, but that’s not actually the case. The validation error is computed at the end of each epoch, while the training error is computed using a running mean during each epoch, so the training curve should be shifted by half an epoch to the left. If you do that, you will see that the training and validation curves overlap almost perfectly at the beginning of training.\n",
    "\n",
    "The training set performance ends up beating the validation performance, as is generally the case when you train for long enough. You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue training. This is as simple as calling the fit() method again, since Keras just continues training where it left off: you should be able to reach about 89.8% validation accuracy, while the training accuracy will continue to rise up to 100% (this is not always the case).\n",
    "\n",
    "If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn’t help, try another optimizer (and always retune the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size (it can be set in the fit() method using the batch_size argument, which defaults to 32). We will get back to hyperparameter tuning at the end of this chapter. Once you are satisfied with your model’s validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this using the evaluate() method (it also supports several other arguments, such as batch_size and sample_weight; please check the documentation for more details):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5297c5e1489d4aeb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4c20e239cff292d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you saw in Chapter 2, it is common to get slightly lower performance on the test set than on the validation set, because the hyperparameters are tuned on the validation set, not the test set (however, in this example, we did not do any hyperparameter tuning, so the lower accuracy is just bad luck). Remember to resist the temptation to tweak the hyperparameters on the test set, or else your estimate of the generalization error will be too optimistic."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "354900817d303ad3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# USING THE MODEL TO MAKE PREDICTIONS\n",
    "Now let’s use the model’s predict() method to make predictions on new instances. Since we don’t have actual new instances, we’ll just use the first three instances of the test set:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6e1b8ab32ff85f6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e9e04ce06d31474"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each instance the model estimates one probability per class, from class 0 to class 9. This is similar to the output of the predict_proba() method in Scikit-Learn classifiers. For example, for the first image it estimates that the probability of class 9 (ankle boot) is 96%, the probability of class 7 (sneaker) is 2%, the probability of class 5 (sandal) is 1%, and the probabilities of the other classes are negligible. In other words, it is highly confident that the first image is footwear, most likely ankle boots but possibly sneakers or sandals. If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the argmax() method to get the highest probability class index for each instance:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6a85a2f74344b54"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = y_proba.argmax(axis=-1)\n",
    "y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24d202caff095119"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "np.array(class_names)[y_pred]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaa57ddaf046f1dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, the classifier actually classified all three images correctly (these images are shown in Figure 10-12):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0daec63209ba9ce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32e975fb4ea1aa85"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Figure 10-12. Correctly classified Fashion MNIST images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d747bac5765ab6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you know how to use the sequential API to build, train, and evaluate a classification MLP. But what about regression?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d9a91fb18f93d45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BUILDING A REGRESSION MLP USING THE SEQUENTIAL API\n",
    "Let’s switch back to the California housing problem and tackle it using the same MLP as earlier, with 3 hidden layers composed of 50 neurons each, but this time building it with Keras.\n",
    "\n",
    "Using the sequential API to build, train, evaluate, and use a regression MLP is quite similar to what we did for classification. The main differences in the following code example are the fact that the output layer has a single neuron (since we only want to predict a single value) and it uses no activation function, the loss function is the mean squared error, the metric is the RMSE, and we’re using an Adam optimizer like Scikit-Learn’s MLPRegressor did. Moreover, in this example we don’t need a Flatten layer, and instead we’re using a Normalization layer as the first layer: it does the same thing as Scikit-Learn’s StandardScaler, but it must be fitted to the training data using its adapt() method before you call the model’s fit() method. (Keras has other preprocessing layers, which will be covered in Chapter 13). Let’s take a look:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6fbee826cdaf95c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3c0be8b50e97674"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "The Normalization layer learns the feature means and standard deviations in the training data when you call the adapt() method. Yet when you display the model’s summary, these statistics are listed as non-trainable. This is because these parameters are not affected by gradient descent."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e0035fa581f7e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the sequential API is quite clean and straightforward. However, although Sequential models are extremely common, it is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs. For this purpose, Keras offers the functional API."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "961e512adb6cb266"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BUILDING COMPLEX MODELS USING THE FUNCTIONAL API\n",
    "One example of a nonsequential neural network is a Wide & Deep neural network. This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng et al.⁠15 It connects all or part of the inputs directly to the output layer, as shown in Figure 10-13. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path).⁠16 In contrast, a regular MLP forces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c0c2eeabfdce778"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-13. A Wide & Deep neural network\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b481e8de404c40db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s build such a neural network to tackle the California housing problem. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bd8f6902ec6a15d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "normalized = normalization_layer(input_)\n",
    "hidden1 = hidden_layer1(normalized)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "concat = concat_layer([normalized, hidden2])\n",
    "output = output_layer(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_], outputs=[output])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b06251eb159f142"
  },
  {
   "cell_type": "markdown",
   "source": [
    "At a high level, the first five lines create all the layers we need to build the model, the next six lines use these layers just like functions to go from the input to the output, and the last line creates a Keras Model object by pointing to the input and the output. Let’s go through this code in more detail:\n",
    "\n",
    "- First, we create five layers: a Normalization layer to standardize the inputs, two Dense layers with 30 neurons each, using the ReLU activation function, a Concatenate layer, and one more Dense layer with a single neuron for the output layer, without any activation function.\n",
    "\n",
    "- Next, we create an Input object (the variable name input_ is used to avoid overshadowing Python’s built-in input() function). This is a specification of the kind of input the model will get, including its shape and optionally its dtype, which defaults to 32-bit floats. A model may actually have multiple inputs, as you will see shortly.\n",
    "\n",
    "- Then we use the Normalization layer just like a function, passing it the Input object. This is why this is called the functional API. Note that we are just telling Keras how it should connect the layers together; no actual data is being processed yet, as the Input object is just a data specification. In other words, it’s a symbolic input. The output of this call is also symbolic: normalized doesn’t store any actual data, it’s just used to construct the model.\n",
    "\n",
    "- In the same way, we then pass normalized to hidden_layer1, which outputs hidden1, and we pass hidden1 to hidden_layer2, which outputs hidden2.\n",
    "\n",
    "- So far we’ve connected the layers sequentially, but then we use the concat_layer to concatenate the input and the second hidden layer’s output. Again, no actual data is concatenated yet: it’s all symbolic, to build the model.\n",
    "\n",
    "- Then we pass concat to the output_layer, which gives us the final output.\n",
    "\n",
    "- Lastly, we create a Keras Model, specifying which inputs and outputs to use.\n",
    "\n",
    "Once you have built this Keras model, everything is exactly like earlier, so there’s no need to repeat it here: you compile the model, adapt the Normalization layer, fit the model, evaluate it, and use it to make predictions.\n",
    "\n",
    "But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path, as illustrated in Figure 10-14? In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4), and six features through the deep path (features 2 to 7). We can do this as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed0b9347b07b56bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\n",
    "input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28abd4238c11aa65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-14. Handling multiple inputs\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73acdf0b139fd3c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are a few things to note in this example, compared to the previous one:\n",
    "\n",
    "Each Dense layer is created and called on the same line. This is a common practice, as it makes the code more concise without losing clarity. However, we can’t do this with the Normalization layer since we need a reference to the layer to be able to call its adapt() method before fitting the model.\n",
    "\n",
    "We used tf.keras.layers.concatenate(), which creates a Concatenate layer and calls it with the given inputs.\n",
    "\n",
    "We specified inputs=[input_wide, input_deep] when creating the model, since there are two inputs.\n",
    "\n",
    "Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_wide, X_train_deep), one per input. The same is true for X_valid, and also for X_test and X_new when you call evaluate() or predict():"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd010cfba3b64f28"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
    "\n",
    "X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n",
    "\n",
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "history = model.fit((X_train_wide, X_train_deep), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_wide, X_valid_deep), y_valid))\n",
    "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
    "y_pred = model.predict((X_new_wide, X_new_deep))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab2353852e13ea0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "Instead of passing a tuple (X_train_wide, X_train_deep), you can pass a dictionary {\"input_wide\": X_train_wide, \"input_deep\": X_train_deep}, if you set name=\"input_wide\" and name=\"input_deep\" when creating the inputs. This is highly recommended when there are many inputs, to clarify the code and avoid getting the order wrong."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d40549bd2166513"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are also many use cases in which you may want to have multiple outputs:\n",
    "\n",
    "- The task may demand it. For instance, you may want to locate and classify the main object in a picture. This is both a regression tasks and a classification task.\n",
    "\n",
    "- Similarly, you may have multiple independent tasks based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks. For example, you could perform multitask classification on pictures of faces, using one output to classify the person’s facial expression (smiling, surprised, etc.) and another output to identify whether they are wearing glasses or not.\n",
    "\n",
    "- Another use case is as a regularization technique (i.e., a training constraint whose objective is to reduce overfitting and thus improve the model’s ability to generalize). For example, you may want to add an auxiliary output in a neural network architecture (see Figure 10-15) to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce1b88615d0595c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-15. Handling multiple outputs, in this example to add an auxiliary output for regularization\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a53172f9072cb565"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding an extra output is quite easy: we just connect it to the appropriate layer and add it to the model’s list of outputs. For example, the following code builds the network represented in Figure 10-15:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0c25d85e1f20b01"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "[...]  # Same as above, up to the main output layer\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "aux_output = tf.keras.layers.Dense(1)(hidden2)\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep],\n",
    "                       outputs=[output, aux_output])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84827f3d6fa4cc1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses. If we pass a single loss, Keras will assume that the same loss must be used for all outputs. By default, Keras will compute all the losses and simply add them up to get the final loss used for training. Since we care much more about the main output than about the auxiliary output (as it is just used for regularization), we want to give the main output’s loss a much greater weight. Luckily, it is possible to set all the loss weights when compiling the model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9577ca3885ef14e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer,\n",
    "              metrics=[\"RootMeanSquaredError\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b29024f262c5632a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "Instead of passing a tuple loss=(\"mse\", \"mse\"), you can pass a dictionary loss={\"output\": \"mse\", \"aux_output\": \"mse\"}, assuming you created the output layers with name=\"output\" and name=\"aux_output\". Just like for the inputs, this clarifies the code and avoids errors when there are several outputs. You can also pass a dictionary for loss_weights."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ac6abac09377492"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now when we train the model, we need to provide labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing y_train, we need to pass (y_train, y_train), or a dictionary {\"output\": y_train, \"aux_output\": y_train} if the outputs were named \"output\" and \"aux_output\". The same goes for y_valid and y_test:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7654821d550da61e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "history = model.fit(\n",
    "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20,\n",
    "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc2d0fbbaae03613"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we evaluate the model, Keras returns the weighted sum of the losses, as well as all the individual losses and metrics:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7435a4f37ef7ffdf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
    "weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0f5c91ceb3d0a02"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "If you set return_dict=True, then evaluate() will return a dictionary instead of a big tuple."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62e4601ec2dfb66d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, the predict() method will return predictions for each output:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b9f3a7589bbf942"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8142e7a6573c4b28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The predict() method returns a tuple, and it does not have a return_dict argument to get a dictionary instead. However, you can create one using model.output_names:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9b8cd7db33982fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y_pred_tuple = model.predict((X_new_wide, X_new_deep))\n",
    "y_pred = dict(zip(model.output_names, y_pred_tuple))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b58d771958dea13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, you can build all sorts of architectures with the functional API. Next, we’ll look at one last way you can build Keras models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a108ff5a60c084e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SUBCLASSING API TO BUILD DYNAMIC MODELS\n",
    "Both the sequential API and the functional API are declarative: you start by declaring which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference. This has many advantages: the model can easily be saved, cloned, and shared; its structure can be displayed and analyzed; the framework can infer shapes and check types, so errors can be caught early (i.e., before any data ever goes through the model). It’s also fairly straightforward to debug, since the whole model is a static graph of layers. But the flip side is just that: it’s static. Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more imperative programming style, the subclassing API is for you.\n",
    "\n",
    "With this approach, you subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, creating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the functional API:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbf3da51d7c00727"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WideAndDeepModel(tf.keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)  # needed to support naming the model\n",
    "        self.norm_layer_wide = tf.keras.layers.Normalization()\n",
    "        self.norm_layer_deep = tf.keras.layers.Normalization()\n",
    "        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = tf.keras.layers.Dense(1)\n",
    "        self.aux_output = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_wide, input_deep = inputs\n",
    "        norm_wide = self.norm_layer_wide(input_wide)\n",
    "        norm_deep = self.norm_layer_deep(input_deep)\n",
    "        hidden1 = self.hidden1(norm_deep)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "        output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e08c003a778fef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example looks like the previous one, except we separate the creation of the layers⁠17 in the constructor from their usage in the call() method. And we don’t need to create the Input objects: we can use the input argument to the call() method.\n",
    "\n",
    "Now that we have a model instance, we can compile it, adapt its normalization layers (e.g., using model.norm_layer_wide.adapt(...) and model.norm_​layer_deep.adapt(...)), fit it, evaluate it, and use it to make predictions, exactly like we did with the functional API.\n",
    "\n",
    "The big difference with this API is that you can include pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations—your imagination is the limit (see Chapter 12)! This makes it a great API when experimenting with new ideas, especially for researchers. However, this extra flexibility does come at a cost: your model’s architecture is hidden within the call() method, so Keras cannot easily inspect it; the model cannot be cloned using tf.keras.models.clone_model(); and when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. So unless you really need that extra flexibility, you should probably stick to the sequential API or the functional API."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0b1b9ee8b340e3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "Keras models can be used just like regular layers, so you can easily combine them to build complex architectures."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7d33b352ffa0f82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that you know how to build and train neural nets using Keras, you will want to save them!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea0552d3e91123a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SAVING AND RESTORING A MODEL\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8a8c5bbd3689121"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model\", save_format=\"tf\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0abf9f8d4cd6979"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you set save_format=\"tf\",18 Keras saves the model using TensorFlow’s SavedModel format: this is a directory (with the given name) containing several files and subdirectories. In particular, the saved_model.pb file contains the model’s architecture and logic in the form of a serialized computation graph, so you don’t need to deploy the model’s source code in order to use it in production; the SavedModel is sufficient (you will see how this works in Chapter 12). The keras_metadata.pb file contains extra information needed by Keras. The variables subdirectory contains all the parameter values (including the connection weights, the biases, the normalization statistics, and the optimizer’s parameters), possibly split across multiple files if the model is very large. Lastly, the assets directory may contain extra files, such as data samples, feature names, class names, and so on. By default, the assets directory is empty. Since the optimizer is also saved, including its hyperparameters and any state it may have, after loading the model you can continue training if you want."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8921ad01acb9d2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "If you set save_format=\"h5\" or use a filename that ends with .h5, .hdf5, or .keras, then Keras will save the model to a single file using a Keras-specific format based on the HDF5 format. However, most TensorFlow deployment tools require the SavedModel format instead."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc918c412e8c125c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will typically have a script that trains a model and saves it, and one or more scripts (or web services) that load the model and use it to evaluate it or to make predictions. Loading the model is just as easy as saving it:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24ba19ffb229e307"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_keras_model\")\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e60bed6e80eb8440"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also use save_weights() and load_weights() to save and load only the parameter values. This includes the connection weights, biases, preprocessing stats, optimizer state, etc. The parameter values are saved in one or more files such as my_weights.data-00004-of-00052, plus an index file like my_weights.index.\n",
    "\n",
    "Saving just the weights is faster and uses less disk space than saving the whole model, so it’s perfect to save quick checkpoints during training. If you’re training a big model, and it takes hours or days, then you must save checkpoints regularly in case the computer crashes. But how can you tell the fit() method to save checkpoints? Use callbacks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d88baf5ed3fb160"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# USING CALLBACKS\n",
    "The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call before and after training, before and after each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d96f6575e29c4274"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_checkpoints\",\n",
    "                                                   save_weights_only=True)\n",
    "history = model.fit([...], callbacks=[checkpoint_cb])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f8333cafb8dc6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moreover, if you use a validation set during training, you can set save_​best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last saved model after training, and this will be the best model on the validation set. This is one way to implement early stopping (introduced in Chapter 4), but it won’t actually stop training.\n",
    "\n",
    "Another way is to use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and if you set restore_best_weights=True it will roll back to the best model at the end of training. You can combine both callbacks to save checkpoints of your model in case your computer crashes, and interrupt training early when there is no more progress, to avoid wasting time and resources and to reduce overfitting:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc4f82151d25b325"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "history = model.fit([...], callbacks=[checkpoint_cb, early_stopping_cb])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b84b8c0eb640b7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of epochs can be set to a large value since training will stop automatically when there is no more progress (just make sure the learning rate is not too small, or else it might keep making slow progress until the end). The EarlyStopping callback will store the weights of the best model in RAM, and it will restore them for you at the end of training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29b1f043687fc62b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Many other callbacks are available in the tf.keras.callbacks package."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc455f6a06c4c06f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you need extra control, you can easily write your own custom callbacks. For example, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g., to detect overfitting):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1d67a3969d633b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        ratio = logs[\"val_loss\"] / logs[\"loss\"]\n",
    "        print(f\"Epoch={epoch}, val/train={ratio:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa46f2125cb72141"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you might expect, you can implement on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), and on_batch_end(). Callbacks can also be used during evaluation and predictions, should you ever need them (e.g., for debugging). For evaluation, you should implement on_test_begin(), on_test_end(), on_test_batch_begin(), or on_test_batch_end(), which are called by evaluate(). For prediction, you should implement on_predict_begin(), on_predict_end(), on_predict_batch_begin(), or on_predict_batch_end(), which are called by predict().\n",
    "\n",
    "Now let’s take a look at one more tool you should definitely have in your toolbox when using Keras: TensorBoard."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2fd8e1e7a2103b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# USING TENSORBOARD FOR VISUALIZATION\n",
    "TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare curves and metrics between multiple runs, visualize the computation graph, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clustered for you, profile your network (i.e., measure its speed to identify bottlenecks), and more!\n",
    "\n",
    "TensorBoard is installed automatically when you install TensorFlow. However, you will need a TensorBoard plug-in to visualize profiling data. If you followed the installation instructions at https://homl.info/install to run everything locally, then you already have the plug-in installed, but if you are using Colab, then you must run the following command:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "252a0dbd70f8af87"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%pip install -q -U tensorboard-plugin-profile"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ed71b1302886bbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use TensorBoard, you must modify your program so that it outputs the data you want to visualize to special binary logfiles called event files. Each binary data record is called a summary. The TensorBoard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations: this allows you to visualize live data (with a short delay), such as the learning curves during training. In general, you want to point the TensorBoard server to a root log directory and configure your program so that it writes to a different subdirectory every time it runs. This way, the same TensorBoard server instance will allow you to visualize and compare data from multiple runs of your program, without getting everything mixed up.\n",
    "\n",
    "Let’s name the root log directory my_logs, and let’s define a little function that generates the path of the log subdirectory based on the current date and time, so that it’s different at every run:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20987ccb8f21173e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import strftime\n",
    "\n",
    "def get_run_logdir(root_logdir=\"my_logs\"):\n",
    "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "run_logdir = get_run_logdir()  # e.g., my_logs/run_2022_08_01_17_25_59"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4be34aa89b5df75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The good news is that Keras provides a convenient TensorBoard() callback that will take care of creating the log directory for you (along with its parent directories if needed), and it will create event files and write summaries to them during training. It will measure your model’s training and validation loss and metrics (in this case, the MSE and RMSE), and it will also profile your neural network. It is straightforward to use:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79211ecef7e49ee1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,\n",
    "                                                profile_batch=(100, 200))\n",
    "history = model.fit([...], callbacks=[tensorboard_cb])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47584794c832606e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "That’s all there is to it! In this example, it will profile the network between batches 100 and 200 during the first epoch. Why 100 and 200? Well, it often takes a few batches for the neural network to “warm up”, so you don’t want to profile too early, and profiling uses resources, so it’s best not to do it for every batch.\n",
    "\n",
    "Next, try changing the learning rate from 0.001 to 0.002, and run the code again, with a new log subdirectory. You will end up with a directory structure similar to this one:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f44cc45a0d96df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# obrazek zde: nejaky textovy strom loggu"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76b6f098ebf2960c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There’s one directory per run, each containing one subdirectory for training logs and one for validation logs. Both contain event files, and the training logs also include profiling traces.\n",
    "\n",
    "Now that you have the event files ready, it’s time to start the TensorBoard server. This can be done directly within Jupyter or Colab using the Jupyter extension for TensorBoard, which gets installed along with the TensorBoard library. This extension is preinstalled in Colab. The following code loads the Jupyter extension for TensorBoard, and the second line starts a TensorBoard server for the my_logs directory, connects to this server and displays the user interface directly inside of Jupyter. The server, listens on the first available TCP port greater than or equal to 6006 (or you can set the port you want using the --port option)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15b1117780807927"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "181a7285a0bb295f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "If you’re running everything on your own machine, it’s possible to start TensorBoard by executing tensorboard --logdir=./my_logs in a terminal. You must first activate the Conda environment in which you installed TensorBoard, and go to the handson-ml3 directory. Once the server is started, visit http://localhost:6006."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b17b6b080a2236c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you should see TensorBoard’s user interface. Click the SCALARS tab to view the learning curves (see Figure 10-16). At the bottom left, select the logs you want to visualize (e.g., the training logs from the first and second run), and click the epoch_loss scalar. Notice that the training loss went down nicely during both runs, but in the second run it went down a bit faster thanks to the higher learning rate."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca9b5ff11fbb9197"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 10-16. Visualizing learning curves with TensorBoard\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75d47c8acb2b7b2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also visualize the whole computation graph in the GRAPHS tab, the learned weights projected to 3D in the PROJECTOR tab, and the profiling traces in the PROFILE tab. The TensorBoard() callback has options to log extra data too (see the documentation for more details). You can click the refresh button (⟳) at the top right to make TensorBoard refresh data, and you can click the settings button (⚙) to activate auto-refresh and specify the refresh interval.\n",
    "\n",
    "Additionally, TensorFlow offers a lower-level API in the tf.summary package. The following code creates a SummaryWriter using the create_file_writer() function, and it uses this writer as a Python context to log scalars, histograms, images, audio, and text, all of which can then be visualized using TensorBoard:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18f05be2d8ca2034"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(str(test_logdir))\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "\n",
    "        data = (np.random.randn(100) + 2) * step / 100  # gets larger\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "\n",
    "        images = np.random.rand(2, 32, 32, 3) * step / 1000  # gets brighter\n",
    "        tf.summary.image(\"my_images\", images, step=step)\n",
    "\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step ** 2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f66a15061d731820"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you run this code and click the refresh button in TensorBoard, you will see several tabs appear: IMAGES, AUDIO, DISTRIBUTIONS, HISTOGRAMS, and TEXT. Try clicking the IMAGES tab, and use the slider above each image to view the images at different time steps. Similarly, go to the AUDIO tab and try listening to the audio at different time steps. As you can see, TensorBoard is a useful tool even beyond TensorFlow or deep learning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2efa5ab7aaf35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "You can share your results online by publishing them to https://tensorboard.dev. For this, just run !tensorboard dev upload --logdir ./my_logs. The first time, it will ask you to accept the terms and conditions and authenticate. Then your logs will be uploaded, and you will get a permanent link to view your results in a TensorBoard interface."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8c7226d499907fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s summarize what you’ve learned so far in this chapter: you now know where neural nets came from, what an MLP is and how you can use it for classification and regression, how to use Keras’s sequential API to build MLPs, and how to use the functional API or the subclassing API to build more complex model architectures (including Wide & Deep models, as well as models with multiple inputs and outputs). You also learned how to save and restore a model and how to use callbacks for checkpointing, early stopping, and more. Finally, you learned how to use TensorBoard for visualization. You can already go ahead and use neural networks to tackle many problems! However, you may wonder how to choose the number of hidden layers, the number of neurons in the network, and all the other hyperparameters. Let’s look at this now."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d24e1085cd46e9d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FINE-TUNING NEURAL NETWORK HYPERPARAMETERS\n",
    "The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak. Not only can you use any imaginable network architecture, but even in a basic MLP you can change the number of layers, the number of neurons and the type of activation function to use in each layer, the weight initialization logic, the type of optimizer to use, its learning rate, the batch size, and more. How do you know what combination of hyperparameters is the best for your task?\n",
    "\n",
    "One option is to convert your Keras model to a Scikit-Learn estimator, and then use GridSearchCV or RandomizedSearchCV to fine-tune the hyperparameters, as you did in Chapter 2. For this, you can use the KerasRegressor and KerasClassifier wrapper classes from the SciKeras library (see https://github.com/adriangb/scikeras for more details). However, there’s a better way: you can use the Keras Tuner library, which is a hyperparameter tuning library for Keras models. It offers several tuning strategies, it’s highly customizable, and it has excellent integration with TensorBoard. Let’s see how to use it.\n",
    "\n",
    "If you followed the installation instructions at https://homl.info/install to run everything locally, then you already have Keras Tuner installed, but if you are using Colab, you’ll need to run %pip install -q -U keras-tuner. Next, import keras_tuner, usually as kt, then write a function that builds, compiles, and returns a Keras model. The function must take a kt.HyperParameters object as an argument, which it can use to define hyperparameters (integers, floats, strings, etc.) along with their range of possible values, and these hyperparameters may be used to build and compile the model. For example, the following function builds and compiles an MLP to classify Fashion MNIST images, using hyperparameters such as the number of hidden layers (n_hidden), the number of neurons per layer (n_neurons), the learning rate (learning_rate), and the type of optimizer to use (optimizer):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eadaf0cdfba0e1fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
    "                             sampling=\"log\")\n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86450d1a963a2e52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first part of the function defines the hyperparameters. For example, hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2) checks whether a hyperparameter named \"n_hidden\" is already present in the HyperParameters object hp, and if so it returns its value. If not, then it registers a new integer hyperparameter named \"n_hidden\", whose possible values range from 0 to 8 (inclusive), and it returns the default value, which is 2 in this case (when default is not set, then min_value is returned). The \"n_neurons\" hyperparameter is registered in a similar way. The \"learning_rate\" hyperparameter is registered as a float ranging from 10–4 to 10–2, and since sampling=\"log\", learning rates of all scales will be sampled equally. Lastly, the optimizer hyperparameter is registered with two possible values: \"sgd\" or \"adam\" (the default value is the first one, which is \"sgd\" in this case). Depending on the value of optimizer, we create an SGD optimizer or an Adam optimizer with the given learning rate.\n",
    "\n",
    "The second part of the function just builds the model using the hyperparameter values. It creates a Sequential model starting with a Flatten layer, followed by the requested number of hidden layers (as determined by the n_hidden hyperparameter) using the ReLU activation function, and an output layer with 10 neurons (one per class) using the softmax activation function. Lastly, the function compiles the model and returns it.\n",
    "\n",
    "Now if you want to do a basic random search, you can create a kt.RandomSearch tuner, passing the build_model function to the constructor, and call the tuner’s search() method:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b999cb162e92ff2e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random_search_tuner = kt.RandomSearch(\n",
    "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
    "    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\n",
    "random_search_tuner.search(X_train, y_train, epochs=10,\n",
    "                           validation_data=(X_valid, y_valid))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13f589a80a30553f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The RandomSearch tuner first calls build_model() once with an empty Hyperparameters object, just to gather all the hyperparameter specifications. Then, in this example, it runs 5 trials; for each trial it builds a model using hyperparameters sampled randomly within their respective ranges, then it trains that model for 10 epochs and saves it to a subdirectory of the my_fashion_mnist/my_rnd_search directory. Since overwrite=True, the my_rnd_search directory is deleted before training starts. If you run this code a second time but with overwrite=False and max_​tri⁠als=10, the tuner will continue tuning where it left off, running 5 more trials: this means you don’t have to run all the trials in one shot. Lastly, since objective is set to \"val_accuracy\", the tuner prefers models with a higher validation accuracy, so once the tuner has finished searching, you can get the best models like this:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10738b07ed8c48fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
    "best_model = top3_models[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b435c1edecf2dd58"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also call get_best_hyperparameters() to get the kt.HyperParameters of the best models:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac89875c8903b46a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n",
    "top3_params[0].values  # best hyperparameter values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c527d4c666c4c01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each tuner is guided by a so-called oracle: before each trial, the tuner asks the oracle to tell it what the next trial should be. The RandomSearch tuner uses a RandomSearchOracle, which is pretty basic: it just picks the next trial randomly, as we saw earlier. Since the oracle keeps track of all the trials, you can ask it to give you the best one, and you can display a summary of that trial:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcfdd736e8eae01c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_trial.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36e0970cf9d4c11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This shows the best hyperparameters (like earlier), as well as the validation accuracy. You can also access all the metrics directly:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ba9917ae4e2e86d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_trial.metrics.get_last_value(\"val_accuracy\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "395423b97d42032a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you are happy with the best model’s performance, you may continue training it for a few epochs on the full training set (X_train_full and y_train_full), then evaluate it on the test set, and deploy it to production (see Chapter 19):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9528570a328be568"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_model.fit(X_train_full, y_train_full, epochs=10)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e496b5536a97b07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In some cases, you may want to fine-tune data preprocessing hyperparameters, or model.fit() arguments, such as the batch size. For this, you must use a slightly different technique: instead of writing a build_model() function, you must subclass the kt.HyperModel class and define two methods, build() and fit(). The build() method does the exact same thing as the build_model() function. The fit() method takes a HyperParameters object and a compiled model as an argument, as well as all the model.fit() arguments, and fits the model and returns the History object. Crucially, the fit() method may use hyperparameters to decide how to preprocess the data, tweak the batch size, and more. For example, the following class builds the same model as before, with the same hyperparameters, but it also uses a Boolean \"normalize\" hyperparameter to control whether or not to standardize the training data before fitting the model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46caeed25f9453c4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MyClassificationHyperModel(kt.HyperModel):\n",
    "    def build(self, hp):\n",
    "        return build_model(hp)\n",
    "\n",
    "    def fit(self, hp, model, X, y, **kwargs):\n",
    "        if hp.Boolean(\"normalize\"):\n",
    "            norm_layer = tf.keras.layers.Normalization()\n",
    "            X = norm_layer(X)\n",
    "        return model.fit(X, y, **kwargs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92effe04a1dc19de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can then pass an instance of this class to the tuner of your choice, instead of passing the build_model function. For example, let’s build a kt.Hyperband tuner based on a MyClassificationHyperModel instance:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50dd7cc77a52a73f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hyperband_tuner = kt.Hyperband(\n",
    "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
    "    max_epochs=10, factor=3, hyperband_iterations=2,\n",
    "    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"hyperband\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "392817f9e3447cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tuner is similar to the HalvingRandomSearchCV class we discussed in Chapter 2: it starts by training many different models for few epochs, then it eliminates the worst models and keeps only the top 1 / factor models (i.e., the top third in this case), repeating this selection process until a single model is left.19 The max_epochs argument controls the max number of epochs that the best model will be trained for. The whole process is repeated twice in this case (hyperband_iterations=2). The total number of training epochs across all models for each hyperband iteration is about max_epochs * (log(max_epochs) / log(factor)) ** 2, so it’s about 44 epochs in this example. The other arguments are the same as for kt.RandomSearch.\n",
    "\n",
    "Let’s run the Hyperband tuner now. We’ll use the TensorBoard callback, this time pointing to the root log directory (the tuner will take care of using a different subdirectory for each trial), as well as an EarlyStopping callback:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89ba14e79ef7bb89"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "hyperband_tuner.search(X_train, y_train, epochs=10,\n",
    "                       validation_data=(X_valid, y_valid),\n",
    "                       callbacks=[early_stopping_cb, tensorboard_cb])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd5ffe1e6bb2645c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now if you open TensorBoard, pointing --logdir to the my_fashion_mnist/hyperband/tensorboard directory, you will see all the trial results as they unfold. Make sure to visit the HPARAMS tab: it contains a summary of all the hyperparameter combinations that were tried, along with the corresponding metrics. Notice that there are three tabs inside the HPARAMS tab: a table view, a parallel coordinates view, and a scatterplot matrix view. In the lower part of the left panel, uncheck all metrics except for validation.epoch_accuracy: this will make the graphs clearer. In the parallel coordinates view, try selecting a range of high values in the validation.epoch_accuracy column: this will filter only the hyperparameter combinations that reached a good performance. Click one of the hyperparameter combinations, and the corresponding learning curves will appear at the bottom of the page. Take some time to go through each tab; this will help you understand the effect of each hyperparameter on performance, as well as the interactions between the hyperparameters.\n",
    "\n",
    "Hyperband is smarter than pure random search in the way it allocates resources, but at its core it still explores the hyperparameter space randomly; it’s fast, but coarse. However, Keras Tuner also includes a kt.BayesianOptimization tuner: this algorithm gradually learns which regions of the hyperparameter space are most promising by fitting a probabilistic model called a Gaussian process. This allows it to gradually zoom in on the best hyperparameters. The downside is that the algorithm has its own hyperparameters: alpha represents the level of noise you expect in the performance measures across trials (it defaults to 10–4), and beta specifies how much you want the algorithm to explore, instead of simply exploiting the known good regions of hyperparameter space (it defaults to 2.6). Other than that, this tuner can be used just like the previous ones:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce142c58d0cc7f2d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bayesian_opt_tuner = kt.BayesianOptimization(\n",
    "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
    "    max_trials=10, alpha=1e-4, beta=2.6,\n",
    "    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"bayesian_opt\")\n",
    "bayesian_opt_tuner.search([...])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbe77ffd3a58b140"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameter tuning is still an active area of research, and many other approaches are being explored. For example, check out DeepMind’s excellent 2017 paper,⁠20 where the authors used an evolutionary algorithm to jointly optimize a population of models and their hyperparameters. Google has also used an evolutionary approach, not just to search for hyperparameters but also to explore all sorts of model architectures: it powers their AutoML service on Google Vertex AI (see Chapter 19). The term AutoML refers to any system that takes care of a large part of the ML workflow. Evolutionary algorithms have even been used successfully to train individual neural networks, replacing the ubiquitous gradient descent! For an example, see the 2017 post by Uber where the authors introduce their Deep Neuroevolution technique.\n",
    "\n",
    "But despite all this exciting progress and all these tools and services, it still helps to have an idea of what values are reasonable for each hyperparameter so that you can build a quick prototype and restrict the search space. The following sections provide guidelines for choosing the number of hidden layers and neurons in an MLP and for selecting good values for some of the main hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a0b19d38fbd0b4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NUMBER OF HIDDEN LAYERS\n",
    "For many problems, you can begin with a single hidden layer and get reasonable results. An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. But for complex problems, deep networks have a much higher parameter efficiency than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data.\n",
    "\n",
    "To understand why, suppose you are asked to draw a forest using some drawing software, but you are forbidden to copy and paste anything. It would take an enormous amount of time: you would have to draw each tree individually, branch by branch, leaf by leaf. If you could instead draw one leaf, copy and paste it to draw a branch, then copy and paste that branch to create a tree, and finally copy and paste this tree to make a forest, you would be finished in no time. Real-world data is often structured in such a hierarchical way, and deep neural networks automatically take advantage of this fact: lower hidden layers model low-level structures (e.g., line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures (e.g., faces).\n",
    "\n",
    "Not only does this hierarchical architecture help DNNs converge faster to a good solution, but it also improves their ability to generalize to new datasets. For example, if you have already trained a model to recognize faces in pictures and you now want to train a new neural network to recognize hairstyles, you can kickstart the training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called transfer learning.\n",
    "\n",
    "In summary, for many problems you can start with just one or two hidden layers and the neural network will work just fine. For instance, you can easily reach above 97% accuracy on the MNIST dataset using just one hidden layer with a few hundred neurons, and above 98% accuracy using two hidden layers with the same total number of neurons, in roughly the same amount of training time. For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set. Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones, as you will see in Chapter 14), and they need a huge amount of training data. You will rarely have to train such networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will then be a lot faster and require much less data (we will discuss this in Chapter 11)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ae6bfd30153d390"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NUMBER OF NEURONS PER HIDDEN LAYER\n",
    "The number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, the MNIST task requires 28 × 28 = 784 inputs and 10 output neurons.\n",
    "\n",
    "As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200, and the third with 100. However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
    "\n",
    "Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. Alternatively, you can try building a model with slightly more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting too much. Vincent Vanhoucke, a scientist at Google, has dubbed this the “stretch pants” approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size. With this approach, you avoid bottleneck layers that could ruin your model. Indeed, if a layer has too few neurons, it will not have enough representational power to preserve all the useful information from the inputs (e.g., a layer with two neurons can only output 2D data, so if it gets 3D data as input, some information will be lost). No matter how big and powerful the rest of the network is, that information will never be recovered."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ad70ff04a691a22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "In general you will get more bang for your buck by increasing the number of layers instead of the number of neurons per layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71bd7b81eb8e934f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LEARNING RATE, BATCH SIZE, AND OTHER HYPERPARAMETERS\n",
    "The number of hidden layers and neurons are not the only hyperparameters you can tweak in an MLP. Here are some of the most important ones, as well as tips on how to set them:\n",
    "\n",
    "**Learning rate**\n",
    "The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges, as we saw in Chapter 4). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., 10–5) and gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g., by (10 / 10-5)1 / 500 to go from 10–5 to 10 in 500 iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point). You can then reinitialize your model and train it normally using this good learning rate. We will look at more learning rate optimization techniques in Chapter 11.\n",
    "\n",
    "**Optimizer**\n",
    "Choosing a better optimizer than plain old mini-batch gradient descent (and tuning its hyperparameters) is also quite important. We will examine several advanced optimizers in Chapter 11.\n",
    "\n",
    "**Batch size**\n",
    "The batch size can have a significant impact on your model’s performance and training time. The main benefit of using large batch sizes is that hardware accelerators like GPUs can process them efficiently (see Chapter 19), so the training algorithm will see more instances per second. Therefore, many researchers and practitioners recommend using the largest batch size that can fit in GPU RAM. There’s a catch, though: in practice, large batch sizes often lead to training instabilities, especially at the beginning of training, and the resulting model may not generalize as well as a model trained with a small batch size. In April 2018, Yann LeCun even tweeted “Friends don’t let friends use mini-batches larger than 32”, citing a 2018 paper⁠21 by Dominic Masters and Carlo Luschi which concluded that using small batches (from 2 to 32) was preferable because small batches led to better models in less training time. Other research points in the opposite direction, however. For example, in 2017, papers by Elad Hoffer et al.⁠22 and Priya Goyal et al.⁠23 showed that it was possible to use very large batch sizes (up to 8,192) along with various techniques such as warming up the learning rate (i.e., starting training with a small learning rate, then ramping it up, as discussed in Chapter 11) and to obtain very short training times, without any generalization gap. So, one strategy is to try to using a large batch size, with learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a small batch size instead.\n",
    "\n",
    "**Activation function**\n",
    "We discussed how to choose the activation function earlier in this chapter: in general, the ReLU activation function will be a good default for all hidden layers, but for the output layer it really depends on your task.\n",
    "\n",
    "**Number of iterations**\n",
    "In most cases, the number of training iterations does not actually need to be tweaked: just use early stopping instead."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5b2a94c34caaf49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "The optimal learning rate depends on the other hyperparameters—especially the batch size—so if you modify any hyperparameter, make sure to update the learning rate as well."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a268951546ce4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more best practices regarding tuning neural network hyperparameters, check out the excellent 2018 paper⁠24 by Leslie Smith.\n",
    "\n",
    "This concludes our introduction to artificial neural networks and their implementation with Keras. In the next few chapters, we will discuss techniques to train very deep nets. We will also explore how to customize models using TensorFlow’s lower-level API and how to load and preprocess data efficiently using the tf.data API. And we will dive into other popular neural network architectures: convolutional neural networks for image processing, recurrent neural networks and transformers for sequential data and text, autoencoders for representation learning, and generative adversarial networks to model and generate data.⁠25"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ff872a5df250a65"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
