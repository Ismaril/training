{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CHAPTER 13. LOADING AND PREPROCESSING DATA WITH TENSORFLOW\n",
    "In Chapter 2, you saw that loading and preprocessing data is an important part of any machine learning project. You used Pandas to load and explore the (modified) California housing dataset—which was stored in a CSV file—and you applied Scikit-Learn’s transformers for preprocessing. These tools are quite convenient, and you will probably be using them often, especially when exploring and experimenting with data.\n",
    "\n",
    "However, when training TensorFlow models on large datasets, you may prefer to use TensorFlow’s own data loading and preprocessing API, called tf.data. It is capable of loading and preprocessing data extremely efficiently, reading from multiple files in parallel using multithreading and queuing, shuffling and batching samples, and more. Plus, it can do all of this on the fly—it loads and preprocesses the next batch of data across multiple CPU cores, while your GPUs or TPUs are busy training the current batch of data.\n",
    "\n",
    "The tf.data API lets you handle datasets that don’t fit in memory, and it allows you to make full use of your hardware resources, thereby speeding up training. Off the shelf, the tf.data API can read from text files (such as CSV files), binary files with fixed-size records, and binary files that use TensorFlow’s TFRecord format, which supports records of varying sizes.\n",
    "\n",
    "TFRecord is a flexible and efficient binary format usually containing protocol buffers (an open source binary format). The tf.data API also has support for reading from SQL databases. Moreover, many open source extensions are available to read from all sorts of data sources, such as Google’s BigQuery service (see https://tensorflow.org/io).\n",
    "\n",
    "Keras also comes with powerful yet easy-to-use preprocessing layers that can be embedded in your models: this way, when you deploy a model to production, it will be able to ingest raw data directly, without you having to add any additional preprocessing code. This eliminates the risk of mismatch between the preprocessing code used during training and the preprocessing code used in production, which would likely cause training/serving skew. And if you deploy your model in multiple apps coded in different programming languages, you won’t have to reimplement the same preprocessing code multiple times, which also reduces the risk of mismatch.\n",
    "\n",
    "As you will see, both APIs can be used jointly—for example, to benefit from the efficient data loading offered by tf.data and the convenience of the Keras preprocessing layers.\n",
    "\n",
    "In this chapter, we will first cover the tf.data API and the TFRecord format. Then we will explore the Keras preprocessing layers and how to use them with the tf.data API. Lastly, we will take a quick look at a few related libraries that you may find useful for loading and preprocessing data, such as TensorFlow Datasets and TensorFlow Hub. So, let’s get started!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2239974fefd445d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# tf.data API\n",
    "The whole tf.data API revolves around the concept of a tf.data.Dataset: this represents a sequence of data items. Usually you will use datasets that gradually read data from disk, but for simplicity let’s create a dataset from a simple data tensor using tf.data.Dataset.from_tensor_slices():"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4c260c6fc08cb6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.range(10)  # any data tensor\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T17:42:27.537839Z",
     "start_time": "2024-03-03T17:42:24.868871Z"
    }
   },
   "id": "d850a99f17c9a448",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X along the first dimension, so this dataset contains 10 items: tensors 0, 1, 2, …​, 9. In this case we would have obtained the same dataset if we had used tf.data.Dataset.range(10) (except the elements would be 64-bit integers instead of 32-bit integers).\n",
    "\n",
    "You can simply iterate over a dataset’s items like this:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18247d5dfb6f7840"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T17:42:27.552161Z",
     "start_time": "2024-03-03T17:42:27.538882Z"
    }
   },
   "id": "a430564592794bee",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "The tf.data API is a streaming API: you can very efficiently iterate through a dataset’s items, but the API is not designed for indexing or slicing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4d5dabc2dfd1bad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A dataset may also contain tuples of tensors, or dictionaries of name/tensor pairs, or even nested tuples and dictionaries of tensors. When slicing a tuple, a dictionary, or a nested structure, the dataset will only slice the tensors it contains, while preserving the tuple/dictionary structure. For example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb33ad176ac1fcce"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=4>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=7>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=5>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=8>}\n",
      "{'a': (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=6>), 'b': <tf.Tensor: shape=(), dtype=int32, numpy=9>}\n"
     ]
    }
   ],
   "source": [
    "X_nested = {\"a\": ([1, 2, 3], [4, 5, 6]), \"b\": [7, 8, 9]}\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_nested)\n",
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T17:42:27.564080Z",
     "start_time": "2024-03-03T17:42:27.552161Z"
    }
   },
   "id": "c8ebcdf0313add7e",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CHAINING TRANSFORMATIONS\n",
    "Once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods. Each method returns a new dataset, so you can chain transformations like this (this chain is illustrated in Figure 13-1):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3393042dbd42e462"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(10))\n",
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T17:42:27.577108Z",
     "start_time": "2024-03-03T17:42:27.564080Z"
    }
   },
   "id": "d5ff1869dbf87c52",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we first call the repeat() method on the original dataset, and it returns a new dataset that repeats the items of the original dataset three times. Of course, this will not copy all the data in memory three times! If you call this method with no arguments, the new dataset will repeat the source dataset forever, so the code that iterates over the dataset will have to decide when to stop.\n",
    "\n",
    "Then we call the batch() method on this new dataset, and again this creates a new dataset. This one will group the items of the previous dataset in batches of seven items.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ecaf88e90ebcb9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 13-1. Chaining dataset transformations\n",
    "![chaining_dataset_transformations](../images/chaining_dataset_transformations.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "707de1911c5d0826"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we iterate over the items of this final dataset. The batch() method had to output a final batch of size two instead of seven, but you can call batch() with drop_remainder=True if you want it to drop this final batch, such that all batches have the exact same size."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af9612c65619db6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "WARNING\n",
    "The dataset methods do not modify datasets—they create new ones. So make sure to keep a reference to these new datasets (e.g., with dataset = ...), or else nothing will happen. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa5dec003bc772c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also transform the items by calling the map() method. For example, this creates a new dataset with all batches multiplied by two:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa54c9f27871a1d8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2)  # x is a batch\n",
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T17:42:27.605321Z",
     "start_time": "2024-03-03T17:42:27.578130Z"
    }
   },
   "id": "7f774eac235990f6",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "This map() method is the one you will call to apply any preprocessing to your data. Sometimes this will include computations that can be quite intensive, such as reshaping or rotating an image, so you will usually want to spawn multiple threads to speed things up. This can be done by setting the num_parallel_calls argument to the number of threads to run, or to tf.data.AUTOTUNE. Note that the function you pass to the map() method must be convertible to a TF function (see Chapter 12).\n",
    "\n",
    "It is also possible to simply filter the dataset using the filter() method. For example, this code creates a dataset that only contains the batchs whose sum is greater than 50:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd854fd6be1a9e0e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: tf.reduce_sum(x) > 50)\n",
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T17:42:27.637680Z",
     "start_time": "2024-03-03T17:42:27.605321Z"
    }
   },
   "id": "f75180494e3defb8",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will often want to look at just a few items from a dataset. You can use the take() method for that:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee72d4368e36c41b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T17:42:27.655673Z",
     "start_time": "2024-03-03T17:42:27.637680Z"
    }
   },
   "id": "42926d5e3b63fbd5",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SHUFFLING THE DATA\n",
    "As we discussed in Chapter 4, gradient descent works best when the instances in the training set are independent and identically distributed (IID). A simple way to ensure this is to shuffle the instances, using the shuffle() method. It will create a new dataset that will start by filling up a buffer with the first items of the source dataset. Then, whenever it is asked for an item, it will pull one out randomly from the buffer and replace it with a fresh one from the source dataset, until it has iterated entirely through the source dataset. At this point it will continue to pull out items randomly from the buffer until it is empty. You must specify the buffer size, and it is important to make it large enough, or else shuffling will not be very effective.⁠1 Just don’t exceed the amount of RAM you have, though even if you have plenty of it, there’s no need to go beyond the dataset’s size. You can provide a random seed if you want the same random order every time you run your program. For example, the following code creates and displays a dataset containing the integers 0 to 9, repeated twice, shuffled using a buffer of size 4 and a random seed of 42, and batched with a batch size of 7:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78b574af59e2d637"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 4 2 3 5 0 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8 2 0 3 1 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 7 9 6 7 8], shape=(6,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(2)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T17:42:27.673252Z",
     "start_time": "2024-03-03T17:42:27.656695Z"
    }
   },
   "id": "3a6f8710ff502b76",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "If you call repeat() on a shuffled dataset, by default it will generate a new order at every iteration. This is generally a good idea, but if you prefer to reuse the same order at each iteration (e.g., for tests or debugging), you can set reshuffle_each_​itera⁠tion=False when calling shuffle()."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23b8d5f66f6cc0dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For a large dataset that does not fit in memory, this simple shuffling-buffer approach may not be sufficient, since the buffer will be small compared to the dataset. One solution is to shuffle the source data itself (for example, on Linux you can shuffle text files using the shuf command). This will definitely improve shuffling a lot! Even if the source data is shuffled, you will usually want to shuffle it some more, or else the same order will be repeated at each epoch, and the model may end up being biased (e.g., due to some spurious patterns present by chance in the source data’s order). To shuffle the instances some more, a common approach is to split the source data into multiple files, then read them in a random order during training. However, instances located in the same file will still end up close to each other. To avoid this you can pick multiple files randomly and read them simultaneously, interleaving their records. Then on top of that you can add a shuffling buffer using the shuffle() method. If this sounds like a lot of work, don’t worry: the tf.data API makes all this possible in just a few lines of code. Let’s go over how you can do this."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd2d015ed1e5bb2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# INTERLEAVING LINES FROM MULTIPLE FILES\n",
    "First, suppose you’ve loaded the California housing dataset, shuffled it (unless it was already shuffled), and split it into a training set, a validation set, and a test set. Then you split each set into many CSV files that each look like this (each row contains eight input features plus the target median house value):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b07168b404ab9bfe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# MedInc,HouseAge,AveRooms,AveBedrms,Popul…,AveOccup,Lat…,Long…,MedianHouseValue\n",
    "# 3.5214,15.0,3.050,1.107,1447.0,1.606,37.63,-122.43,1.442\n",
    "# 5.3275,5.0,6.490,0.991,3464.0,3.443,33.69,-117.39,1.687\n",
    "# 3.1,29.0,7.542,1.592,1328.0,2.251,38.44,-122.98,1.621\n",
    "# [...]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f19e93443d0664b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s also suppose train_filepaths contains the list of training filepaths (and you also have valid_filepaths and test_filepaths):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "840fb368b4c33156"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_filepaths"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b400a979eb26396"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alternatively, you could use file patterns; for example, train_filepaths = \"datasets/housing/my_train_*.csv\". Now let’s create a dataset containing only these filepaths:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4989674128db8d83"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e2ab9ae4d4d0360"
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, the list_files() function returns a dataset that shuffles the filepaths. In general this is a good thing, but you can set shuffle=False if you do not want that for some reason.\n",
    "\n",
    "Next, you can call the interleave() method to read from five files at a time and interleave their lines. You can also skip the first line of each file—which is the header row—using the skip() method):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1953b188e92ebcbb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ac075ac68f9a1f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The interleave() method will create a dataset that will pull five filepaths from the filepath_dataset, and for each one it will call the function you gave it (a lambda in this example) to create a new dataset (in this case a TextLineDataset). To be clear, at this stage there will be seven datasets in all: the filepath dataset, the interleave dataset, and the five TextLineDatasets created internally by the interleave dataset. When you iterate over the interleave dataset, it will cycle through these five TextLineDatasets, reading one line at a time from each until all datasets are out of items. Then it will fetch the next five filepaths from the filepath_dataset and interleave them the same way, and so on until it runs out of filepaths. For interleaving to work best, it is preferable to have files of identical length; otherwise the end of the longest file will not be interleaved.\n",
    "\n",
    "By default, interleave() does not use parallelism; it just reads one line at a time from each file, sequentially. If you want it to actually read files in parallel, you can set the interleave() method’s num_parallel_calls argument to the number of threads you want (recall that the map() method also has this argument). You can even set it to tf.data.AUTOTUNE to make TensorFlow choose the right number of threads dynamically based on the available CPU. Let’s look at what the dataset contains now:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "952da5ec7ddcd3a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25e40126f9a65121"
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are the first rows (ignoring the header row) of five CSV files, chosen randomly. Looks good!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4326c4b9a051eb1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "It’s possible to pass a list of filepaths to the TextLineDataset constructor: it will go through each file in order, line by line. If you also set the num_parallel_reads argument to a number greater than one, then the dataset will read that number of files in parallel and interleave their lines (without having to call the interleave() method). However, it will not shuffle the files, nor will it skip the header lines."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "616f689a1d7d6adb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PREPROCESSING THE DATA\n",
    "Now that we have a housing dataset that returns each instance as a tensor containing a byte string, we need to do a bit of preprocessing, including parsing the strings and scaling the data. Let’s implement a couple custom functions that will perform this preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41f7561ccee52677"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_mean, X_std = [...]  # mean and scale of each feature in the training set\n",
    "n_inputs = 8\n",
    "\n",
    "def parse_csv_line(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])\n",
    "\n",
    "def preprocess(line):\n",
    "    x, y = parse_csv_line(line)\n",
    "    return (x - X_mean) / X_std, y"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb72d16946bcb43f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s walk through this code:\n",
    "\n",
    "- First, the code assumes that we have precomputed the mean and standard deviation of each feature in the training set. X_mean and X_std are just 1D tensors (or NumPy arrays) containing eight floats, one per input feature. This can be done using a Scikit-Learn StandardScaler on a large enough random sample of the dataset. Later in this chapter, we will use a Keras preprocessing layer instead.\n",
    "\n",
    "- The parse_csv_line() function takes one CSV line and parses it. To help with that, it uses the tf.io.decode_csv() function, which takes two arguments: the first is the line to parse, and the second is an array containing the default value for each column in the CSV file. This array (defs) tells TensorFlow not only the default value for each column, but also the number of columns and their types. In this example, we tell it that all feature columns are floats and that missing values should default to zero, but we provide an empty array of type tf.float32 as the default value for the last column (the target): the array tells TensorFlow that this column contains floats, but that there is no default value, so it will raise an exception if it encounters a missing value.\n",
    "\n",
    "- The tf.io.decode_csv() function returns a list of scalar tensors (one per column), but we need to return a 1D tensor array. So we call tf.stack() on all tensors except for the last one (the target): this will stack these tensors into a 1D array. We then do the same for the target value: this makes it a 1D tensor array with a single value, rather than a scalar tensor. The tf.io.decode_csv() function is done, so it returns the input features and the target.\n",
    "\n",
    "- Finally, the custom preprocess() function just calls the parse_csv_line() function, scales the input features by subtracting the feature means and then dividing by the feature standard deviations, and returns a tuple containing the scaled features and the target.\n",
    "\n",
    "Let’s test this preprocessing function:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7ba9b3462644332"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69edb742139f919b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks good! The preprocess() function can convert an instance from a byte string to a nice scaled tensor, with its corresponding label. We can now use the dataset’s map() method to apply the preprocess() function to each sample in the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d655e3851e63375"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PUTTING EVERYTHING TOGETHER\n",
    "To make the code more reusable, let’s put together everything we have discussed so far into another helper function; it will create and return a dataset that will efficiently load California housing data from multiple CSV files, preprocess it, shuffle it, and batch it (see Figure 13-2):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bfa2f46073cecdc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, n_readers=5, n_read_threads=None,\n",
    "                       n_parse_threads=5, shuffle_buffer_size=10_000, seed=42,\n",
    "                       batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, seed=seed)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size, seed=seed)\n",
    "    return dataset.batch(batch_size).prefetch(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aada978c6bb5699b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that we use the prefetch() method on the very last line. This is important for performance, as you will see now."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a8f612e412d46b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 13-2. Loading and preprocessing data from multiple CSV files\n",
    "![loading_and_preprocessing_data_from_multiple_CSV_files](../images/loading_and_preprocessing_data_from_multiple_CSV_files.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5ede10c77f099a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PREFETCHING\n",
    "By calling prefetch(1) at the end of the custom csv_reader_dataset() function, we are creating a dataset that will do its best to always be one batch ahead.⁠2 In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready (e.g., reading the data from disk and preprocessing it). This can improve performance dramatically, as is illustrated in Figure 13-3.\n",
    "\n",
    "If we also ensure that loading and preprocessing are multithreaded (by setting num_parallel_calls when calling interleave() and map()), we can exploit multiple CPU cores and hopefully make preparing one batch of data shorter than running a training step on the GPU: this way the GPU will be almost 100% utilized (except for the data transfer time from the CPU to the GPU⁠3), and training will run much faster."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "668b8ed761c08dea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 13-3. With prefetching, the CPU and the GPU work in parallel: as the GPU works on one batch, the CPU works on the next\n",
    "![mls3_1303](../images/mls3_1303.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cee495d33048d597"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "If you plan to purchase a GPU card, its processing power and its memory size are of course very important (in particular, a large amount of RAM is crucial for large computer vision or natural language processing models). Just as important for good performance is the GPU’s memory bandwidth; this is the number of gigabytes of data it can get into or out of its RAM per second."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52345b0b36282829"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the dataset is small enough to fit in memory, you can significantly speed up training by using the dataset’s cache() method to cache its content to RAM. You should generally do this after loading and preprocessing the data, but before shuffling, repeating, batching, and prefetching. This way, each instance will only be read and preprocessed once (instead of once per epoch), but the data will still be shuffled differently at each epoch, and the next batch will still be prepared in advance.\n",
    "\n",
    "You have now learned how to build efficient input pipelines to load and preprocess data from multiple text files. We have discussed the most common dataset methods, but there are a few more you may want to look at, such as concatenate(), zip(), window(), reduce(), shard(), flat_map(), apply(), unbatch(), and padded_batch(). There are also a few more class methods, such as from_generator() and from_​ten⁠sors(), which create a new dataset from a Python generator or a list of tensors, respectively. Please check the API documentation for more details. Also note that there are experimental features available in tf.data.experimental, many of which will likely make it to the core API in future releases (e.g., check out the CsvDataset class, as well as the make_csv_dataset() method, which takes care of inferring the type of each column)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b9b04bbe91dc964"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# USING THE DATASET WITH KERAS\n",
    "Now we can use the custom csv_reader_dataset() function we wrote earlier to create a dataset for the training set, and for the validation set and the test set. The training set will be shuffled at each epoch (note that the validation set and the test set will also be shuffled, even though we don’t really need that):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4682511f75c9e1dd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b6f0062e0a068c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you can simply build and train a Keras model using these datasets. When you call the model’s fit() method, you pass train_set instead of X_train, y_train, and pass validation_data=valid_set instead of validation_data=(X_valid, y_valid). The fit() method will take care of repeating the training dataset once per epoch, using a different random order at each epoch:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74fd302316406f15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([...])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(train_set, validation_data=valid_set, epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74ea30c35887c54b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, you can pass a dataset to the evaluate() and predict() methods:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3745234e1ffa3e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_mse = model.evaluate(test_set)\n",
    "new_set = test_set.take(3)  # pretend we have 3 new samples\n",
    "y_pred = model.predict(new_set)  # or you could just pass a NumPy array"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b975d98bd003d0af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unlike the other sets, the new_set will usually not contain labels. If it does, as is the case here, Keras will ignore them. Note that in all these cases, you can still use NumPy arrays instead of datasets if you prefer (but of course they need to have been loaded and preprocessed first).\n",
    "\n",
    "If you want to build your own custom training loop (as discussed in Chapter 12), you can just iterate over the training set, very naturally:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9a14b25ccea8ef5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        [...]  # perform one gradient descent step"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdc3e6ba022d537a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In fact, it is even possible to create a TF function (see Chapter 12) that trains the model for a whole epoch. This can really speed up training:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "414b56bacff3496b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_one_epoch(model, optimizer, loss_fn, train_set):\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"\\rEpoch {}/{}\".format(epoch + 1, n_epochs), end=\"\")\n",
    "    train_one_epoch(model, optimizer, loss_fn, train_set)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db09de3553ea5cb7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In Keras, the steps_per_execution argument of the compile() method lets you define the number of batches that the fit() method will process during each call to the tf.function it uses for training. The default is just 1, so if you set it to 50 you will often see a significant performance improvement. However, the on_batch_*() methods of Keras callbacks will only be called every 50 batches.\n",
    "\n",
    "Congratulations, you now know how to build powerful input pipelines using the tf.data API! However, so far we’ve been using CSV files, which are common, simple, and convenient but not really efficient, and do not support large or complex data structures (such as images or audio) very well. So, let’s see how to use TFRecords instead."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17a05997516c0d81"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "If you are happy with CSV files (or whatever other format you are using), you do not have to use TFRecords. As the saying goes, if it ain’t broke, don’t fix it! TFRecords are useful when the bottleneck during training is loading and parsing the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5182c7adeee3f594"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TFRecord FORMAT\n",
    "The TFRecord format is TensorFlow’s preferred format for storing large amounts of data and reading it efficiently. It is a very simple binary format that just contains a sequence of binary records of varying sizes (each record is comprised of a length, a CRC checksum to check that the length was not corrupted, then the actual data, and finally a CRC checksum for the data). You can easily create a TFRecord file using the tf.io.TFRecordWriter class:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "994317f782ca57d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ab43729af3bf398"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And you can then use a tf.data.TFRecordDataset to read one or more TFRecord files:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14a42dee7adbf584"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 178: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m filepaths \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmy_data.tfrecord\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mTFRecordDataset(filepaths)\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mitem\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:814\u001B[0m, in \u001B[0;36mOwnedIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    813\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 814\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    815\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mOutOfRangeError:\n\u001B[0;32m    816\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:777\u001B[0m, in \u001B[0;36mOwnedIterator._next_internal\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    774\u001B[0m \u001B[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001B[39;00m\n\u001B[0;32m    775\u001B[0m \u001B[38;5;66;03m# to communicate that there is no more data to iterate over.\u001B[39;00m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecution_mode(context\u001B[38;5;241m.\u001B[39mSYNC):\n\u001B[1;32m--> 777\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator_get_next\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    778\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator_resource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    779\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_output_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    780\u001B[0m \u001B[43m      \u001B[49m\u001B[43moutput_shapes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_output_shapes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_element_spec\u001B[38;5;241m.\u001B[39m_from_compatible_tensor_list(ret)  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3050\u001B[0m, in \u001B[0;36miterator_get_next\u001B[1;34m(iterator, output_types, output_shapes, name)\u001B[0m\n\u001B[0;32m   3048\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[0;32m   3049\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3050\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3051\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mIteratorGetNext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput_types\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3052\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput_shapes\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_shapes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3053\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m   3054\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xe9 in position 178: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:12:42.668689Z",
     "start_time": "2024-03-03T21:12:41.219677Z"
    }
   },
   "id": "f8b8415256cb8d1d",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "This will output:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d82260c4e6f16572"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
    "tf.Tensor(b'And this is the second record', shape=(), dtype=string)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37cf1525bc5fcb46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "By default, a TFRecordDataset will read files one by one, but you can make it read multiple files in parallel and interleave their records by passing the constructor a list of filepaths and setting num_parallel_reads to a number greater than one. Alternatively, you could obtain the same result by using list_files() and interleave() as we did earlier to read multiple CSV files."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8efc193e8b302a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# COMPRESSED TFRecord FILES"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de8820547273581e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can sometimes be useful to compress your TFRecord files, especially if they need to be loaded via a network connection. You can create a compressed TFRecord file by setting the options argument:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df68f768d9f56ecc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"Compress, compress, compress!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2becb00916264a6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When reading a compressed TFRecord file, you need to specify the compression type:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1429207b1f846923"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
    "                                  compression_type=\"GZIP\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b4c471d15cffc5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A BRIEF INTRODUCTION TO PROTOCOL BUFFERS\n",
    "Even though each record can use any binary format you want, TFRecord files usually contain serialized protocol buffers (also called protobufs). This is a portable, extensible, and efficient binary format developed at Google back in 2001 and made open source in 2008; protobufs are now widely used, in particular in gRPC, Google’s remote procedure call system. They are defined using a simple language that looks like this:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fa7e3433c1d1082"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "syntax = \"proto3\";\n",
    "message Person {\n",
    "    string name = 1;\n",
    "    int32 id = 2;\n",
    "    repeated string email = 3;\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a36f0e5a1657b1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This protobuf definition says we are using version 3 of the protobuf format, and it specifies that each Person object⁠4 may (optionally) have a name of type string, an id of type int32, and zero or more email fields, each of type string. The numbers 1, 2, and 3 are the field identifiers: they will be used in each record’s binary representation. Once you have a definition in a .proto file, you can compile it. This requires protoc, the protobuf compiler, to generate access classes in Python (or some other language). Note that the protobuf definitions you will generally use in TensorFlow have already been compiled for you, and their Python classes are part of the TensorFlow library, so you will not need to use protoc. All you need to know is how to use protobuf access classes in Python. To illustrate the basics, let’s look at a simple example that uses the access classes generated for the Person protobuf (the code is explained in the comments):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed28a6c1be7e73b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    ">>> from person_pb2 import Person  # import the generated access class\n",
    ">>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\n",
    ">>> print(person)  # display the Person\n",
    "name: \"Al\"\n",
    "id: 123\n",
    "email: \"a@b.com\"\n",
    ">>> person.name  # read a field\n",
    "'Al'\n",
    ">>> person.name = \"Alice\"  # modify a field\n",
    ">>> person.email[0]  # repeated fields can be accessed like arrays\n",
    "'a@b.com'\n",
    ">>> person.email.append(\"c@d.com\")  # add an email address\n",
    ">>> serialized = person.SerializeToString()  # serialize person to a byte string\n",
    ">>> serialized\n",
    "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n",
    ">>> person2 = Person()  # create a new Person\n",
    ">>> person2.ParseFromString(serialized)  # parse the byte string (27 bytes long)\n",
    "27\n",
    ">>> person == person2  # now they are equal\n",
    "True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98b30ce93b47c4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In short, we import the Person class generated by protoc, we create an instance and play with it, visualizing it and reading and writing some fields, then we serialize it using the SerializeToString() method. This is the binary data that is ready to be saved or transmitted over the network. When reading or receiving this binary data, we can parse it using the ParseFromString() method, and we get a copy of the object that was serialized.⁠5\n",
    "\n",
    "You could save the serialized Person object to a TFRecord file, then load and parse it: everything would work fine. However, ParseFromString() is not a TensorFlow operation, so you couldn’t use it in a preprocessing function in a tf.data pipeline (except by wrapping it in a tf.py_function() operation, which would make the code slower and less portable, as you saw in Chapter 12). However, you could use the tf.io.decode_proto() function, which can parse any protobuf you want, provided you give it the protobuf definition (see the notebook for an example). That said, in practice you will generally want to use instead the predefined protobufs for which TensorFlow provides dedicated parsing operations. Let’s look at these predefined protobufs now."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5498d83a6a4ecf0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TENSORFLOW PROTOBUFS\n",
    "The main protobuf typically used in a TFRecord file is the Example protobuf, which represents one instance in a dataset. It contains a list of named features, where each feature can either be a list of byte strings, a list of floats, or a list of integers. Here is the protobuf definition (from TensorFlow’s source code):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c39264496342e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "syntax = \"proto3\";\n",
    "message BytesList { repeated bytes value = 1; }\n",
    "message FloatList { repeated float value = 1 [packed = true]; }\n",
    "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
    "message Feature {\n",
    "    oneof kind {\n",
    "        BytesList bytes_list = 1;\n",
    "        FloatList float_list = 2;\n",
    "        Int64List int64_list = 3;\n",
    "    }\n",
    "};\n",
    "message Features { map<string, Feature> feature = 1; };\n",
    "message Example { Features features = 1; };"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1544e79ccf0fa733"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The definitions of BytesList, FloatList, and Int64List are straightforward enough. Note that [packed = true] is used for repeated numerical fields, for a more efficient encoding. A Feature contains either a BytesList, a FloatList, or an Int64List. A Features (with an s) contains a dictionary that maps a feature name to the corresponding feature value. And finally, an Example contains only a Features object."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9a1a13aa2e8cc34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "Why was Example even defined, since it contains no more than a Features object? Well, TensorFlow’s developers may one day decide to add more fields to it. As long as the new Example definition still contains the features field, with the same ID, it will be backward compatible. This extensibility is one of the great features of protobufs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4e2a4dbe6708452"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is how you could create a tf.train.Example representing the same person as earlier:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "225d6b72ac692ef5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
    "                                                          b\"c@d.com\"]))\n",
    "        }))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e83c5cb858ad5491"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code is a bit verbose and repetitive, but you could easily wrap it inside a small helper function. Now that we have an Example protobuf, we can serialize it by calling its SerializeToString() method, then write the resulting data to a TFRecord file. Let’s write it five times to pretend we have several contacts:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e0670d433680da5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    for _ in range(5):\n",
    "        f.write(person_example.SerializeToString())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9961b19fe903539"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normally you would write much more than five Examples! Typically, you would create a conversion script that reads from your current format (say, CSV files), creates an Example protobuf for each instance, serializes them, and saves them to several TFRecord files, ideally shuffling them in the process. This requires a bit of work, so once again make sure it is really necessary (perhaps your pipeline works fine with CSV files).\n",
    "\n",
    "Now that we have a nice TFRecord file containing several serialized Examples, let’s try to load it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52e1b635160ee026"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOADING AND PARSING EXAMPLES\n",
    "To load the serialized Example protobufs, we will use a tf.data.TFRecordDataset once again, and we will parse each Example using tf.io.parse_single_example(). It requires at least two arguments: a string scalar tensor containing the serialized data, and a description of each feature. The description is a dictionary that maps each feature name to either a tf.io.FixedLenFeature descriptor indicating the feature’s shape, type, and default value, or a tf.io.VarLenFeature descriptor indicating only the type if the length of the feature’s list may vary (such as for the \"emails\" feature).\n",
    "\n",
    "The following code defines a description dictionary, then creates a TFRecordDataset and applies a custom preprocessing function to it to parse each serialized Example protobuf that this dataset contains:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e278bf5aa1ed9d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "\n",
    "def parse(serialized_example):\n",
    "    return tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).map(parse)\n",
    "for parsed_example in dataset:\n",
    "    print(parsed_example)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59af5de1be4fd9b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The fixed-length features are parsed as regular tensors, but the variable-length features are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor using tf.sparse.to_dense(), but in this case it is simpler to just access its values:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55a4cc19df29de54"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d41b6d54ed3c7052"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parsed_example[\"emails\"].values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c66d80ef5b38ddc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of parsing examples one by one using tf.io.parse_single_example(), you may want to parse them batch by batch using tf.io.parse_example():"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f2430c731f61c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def parse(serialized_examples):\n",
    "    return tf.io.parse_example(serialized_examples, feature_description)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(2).map(parse)\n",
    "for parsed_examples in dataset:\n",
    "    print(parsed_examples)  # two examples at a time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f883a99273a2bbc3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, a BytesList can contain any binary data you want, including any serialized object. For example, you can use tf.io.encode_jpeg() to encode an image using the JPEG format and put this binary data in a BytesList. Later, when your code reads the TFRecord, it will start by parsing the Example, then it will need to call tf.io.decode_jpeg() to parse the data and get the original image (or you can use tf.io.decode_image(), which can decode any BMP, GIF, JPEG, or PNG image). You can also store any tensor you want in a BytesList by serializing the tensor using tf.io.serialize_tensor() then putting the resulting byte string in a BytesList feature. Later, when you parse the TFRecord, you can parse this data using tf.io.parse_tensor(). See this chapter’s notebook at https://homl.info/colab3 for examples of storing images and tensors in a TFRecord file.\n",
    "\n",
    "As you can see, the Example protobuf is quite flexible, so it will probably be sufficient for most use cases. However, it may be a bit cumbersome to use when you are dealing with lists of lists. For example, suppose you want to classify text documents. Each document may be represented as a list of sentences, where each sentence is represented as a list of words. And perhaps each document also has a list of comments, where each comment is represented as a list of words. There may be some contextual data too, such as the document’s author, title, and publication date. TensorFlow’s SequenceExample protobuf is designed for such use cases."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c496ccc563ba7a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# HANDLING LISTS OF LISTS USING THE SEQUENCEEXAMPLE PROTOBUF\n",
    "Here is the definition of the SequenceExample protobuf:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98c35f7379f1820f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "message FeatureList { repeated Feature feature = 1; };\n",
    "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
    "message SequenceExample {\n",
    "    Features context = 1;\n",
    "    FeatureLists feature_lists = 2;\n",
    "};"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2a67c492bb7dbb6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "A SequenceExample contains a Features object for the contextual data and a FeatureLists object that contains one or more named FeatureList objects (e.g., a FeatureList named \"content\" and another named \"comments\"). Each FeatureList contains a list of Feature objects, each of which may be a list of byte strings, a list of 64-bit integers, or a list of floats (in this example, each Feature would represent a sentence or a comment, perhaps in the form of a list of word identifiers). Building a SequenceExample, serializing it, and parsing it is similar to building, serializing, and parsing an Example, but you must use tf.io.parse_single_sequence_example() to parse a single SequenceExample or tf.io.parse_sequence_example() to parse a batch. Both functions return a tuple containing the context features (as a dictionary) and the feature lists (also as a dictionary). If the feature lists contain sequences of varying sizes (as in the preceding example), you may want to convert them to ragged tensors using tf.RaggedTensor.from_sparse() (see the notebook for the full code):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18ee2d63a6015183"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
    "    serialized_sequence_example, context_feature_descriptions,\n",
    "    sequence_feature_descriptions)\n",
    "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b757c2483135780"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that you know how to efficiently store, load, parse, and preprocess the data using the tf.data API, TFRecords, and protobufs, it’s time to turn our attention to the Keras preprocessing layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88ea5b86b0c345c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# KERAS PREPROCESSING LAYERS\n",
    "Preparing your data for a neural network typically requires normalizing the numerical features, encoding the categorical features and text, cropping and resizing images, and more. There are several options for this:\n",
    "\n",
    "- The preprocessing can be done ahead of time when preparing your training data files, using any tools you like, such as NumPy, Pandas, or Scikit-Learn. You will need to apply the exact same preprocessing steps in production, to ensure your production model receives preprocessed inputs similar to the ones it was trained on.\n",
    "\n",
    "- Alternatively, you can preprocess your data on the fly while loading it with tf.data, by applying a preprocessing function to every element of a dataset using that dataset’s map() method, as we did earlier in this chapter. Again, you will need to apply the same preprocessing steps in production.\n",
    "\n",
    "- One last approach is to include preprocessing layers directly inside your model so it can preprocess all the input data on the fly during training, then use the same preprocessing layers in production. The rest of this chapter will look at this last approach.\n",
    "\n",
    "Keras offers many preprocessing layers that you can include in your models: they can be applied to numerical features, categorical features, images, and text. We’ll go over the numerical and categorical features in the next sections, as well as basic text preprocessing, and we will cover image preprocessing in Chapter 14 and more advanced text preprocessing in Chapter 16."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "630f0903a5c916a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE NORMALIZATION LAYER\n",
    "As we saw in Chapter 10, Keras provides a Normalization layer that we can use to standardize the input features. We can either specify the mean and variance of each feature when creating the layer or—more simply—pass the training set to the layer’s adapt() method before fitting the model, so the layer can measure the feature means and variances on its own before training:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55f9937229a32c28"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "model = tf.keras.models.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "norm_layer.adapt(X_train)  # computes the mean and variance of every feature\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e0e0c59c70b97b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "The data sample passed to the adapt() method must be large enough to be representative of your dataset, but it does not have to be the full training set: for the Normalization layer, a few hundred instances randomly sampled from the training set will generally be sufficient to get a good estimate of the feature means and variances."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4481375edba090d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we included the Normalization layer inside the model, we can now deploy this model to production without having to worry about normalization again: the model will just handle it (see Figure 13-4). Fantastic! This approach completely eliminates the risk of preprocessing mismatch, which happens when people try to maintain different preprocessing code for training and production but update one and forget to update the other. The production model then ends up receiving data preprocessed in a way it doesn’t expect. If they’re lucky, they get a clear bug. If not, the model’s accuracy just silently degrades."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3bc5d172092ad98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 13-4. Including preprocessing layers inside a model\n",
    "![including_preprocessing_layers_inside_a_model](../images/including_preprocessing_layers_inside_a_model.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4f11a61f5b5d5cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Including the preprocessing layer directly in the model is nice and straightforward, but it will slow down training (only very slightly in the case of the Normalization layer): indeed, since preprocessing is performed on the fly during training, it happens once per epoch. We can do better by normalizing the whole training set just once before training. To do this, we can use the Normalization layer in a standalone fashion (much like a Scikit-Learn StandardScaler):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f98dd03b07048d0b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "X_train_scaled = norm_layer(X_train)\n",
    "X_valid_scaled = norm_layer(X_valid)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b790787b1f618f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can train a model on the scaled data, this time without a Normalization layer:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a698c5cb16585d51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=2e-3))\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42abe0e6b50ae8a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Good! This should speed up training a bit. But now the model won’t preprocess its inputs when we deploy it to production. To fix this, we just need to create a new model that wraps both the adapted Normalization layer and the model we just trained. We can then deploy this final model to production, and it will take care of both preprocessing its inputs and making predictions (see Figure 13-5):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74333527260dfc72"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_model = tf.keras.Sequential([norm_layer, model])\n",
    "X_new = X_test[:3]  # pretend we have a few new instances (unscaled)\n",
    "y_pred = final_model(X_new)  # preprocesses the data and makes predictions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa2512c6f8062f9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 13-5. Preprocessing the data just once before training using preprocessing layers, then deploying these layers inside the final model\n",
    "![mls3_1305](../images/mls3_1305.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13afcf9208fa9367"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have the best of both worlds: training is fast because we only preprocess the data once before training begins, and the final model can preprocess its inputs on the fly without any risk of preprocessing mismatch.\n",
    "\n",
    "Moreover, the Keras preprocessing layers play nicely with the tf.data API. For example, it’s possible to pass a tf.data.Dataset to a preprocessing layer’s adapt() method. It’s also possible to apply a Keras preprocessing layer to a tf.data.Dataset using the dataset’s map() method. For example, here’s how you could apply an adapted Normalization layer to the input features of each batch in a dataset:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a116fc7385f9fdb0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda X, y: (norm_layer(X), y))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9563e4f97c7e0eec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, if you ever need more features than the Keras preprocessing layers provide, you can always write your own Keras layer, just like we discussed in Chapter 12. For example, if the Normalization layer didn’t exist, you could get a similar result using the following custom layer:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2e164cc04a6ec43"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyNormalization(tf.keras.layers.Layer):\n",
    "    def adapt(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
    "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        eps = tf.keras.backend.epsilon()  # a small smoothing term\n",
    "        return (inputs - self.mean_) / (self.std_ + eps)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d25e37e99a02ee9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, let’s look at another Keras preprocessing layer for numerical features: the Discretization layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "615f002d3b1f439a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE DISCRETIZATION LAYER\n",
    "The Discretization layer’s goal is to transform a numerical feature into a categorical feature by mapping value ranges (called bins) to categories. This is sometimes useful for features with multimodal distributions, or with features that have a highly non-linear relationship with the target. For example, the following code maps a numerical age feature to three categories, less than 18, 18 to 50 (not included), and 50 or over:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e48a0dc9d19df888"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "age = tf.constant([[10.], [93.], [57.], [18.], [37.], [5.]])\n",
    "discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "530d6d37bee817ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we provided the desired bin boundaries. If you prefer, you can instead provide the number of bins you want, then call the layer’s adapt() method to let it find the appropriate bin boundaries based on the value percentiles. For example, if we set num_bins=3, then the bin boundaries will be located at the values just below the 33rd and 66th percentiles (in this example, at the values 10 and 37):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f65f931052761c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
    "discretize_layer.adapt(age)\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77b4300ad0cbb3d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Category identifiers such as these should generally not be passed directly to a neural network, as their values cannot be meaningfully compared. Instead, they should be encoded, for example using one-hot encoding. Let’s look at how to do this now."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa37ff749bf9cb9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE CATEGORYENCODING LAYER\n",
    "When there are only a few categories (e.g., less than a dozen or two), then one-hot encoding is often a good option (as discussed in Chapter 2). To do this, Keras provides the CategoryEncoding layer. For example, let’s one-hot encode the age_​cate⁠gories feature we just created:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9470bf5199b9a9d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
    "onehot_layer(age_categories)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c29c07fe57e7881"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you try to encode more than one categorical feature at a time (which only makes sense if they all use the same categories), the CategoryEncoding class will perform multi-hot encoding by default: the output tensor will contain a 1 for each category present in any input feature. For example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23e433749cb93796"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "two_age_categories = np.array([[1, 0], [2, 2], [2, 0]])\n",
    "onehot_layer(two_age_categories)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbb8ad27a5789b22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you believe it’s useful to know how many times each category occurred, you can set output_mode=\"count\" when creating the CategoryEncoding layer, in which case the output tensor will contain the number of occurrences of each category. In the preceding example, the output would be the same except for the second row, which would become [0., 0., 2.].\n",
    "\n",
    "Note that both multi-hot encoding and count encoding lose information, since it’s not possible to know which feature each active category came from. For example, both [0, 1] and [1, 0] are encoded as [1., 1., 0.]. If you want to avoid this, then you need to one-hot encode each feature separately and concatenate the outputs. This way, [0, 1] would get encoded as [1., 0., 0., 0., 1., 0.] and [1, 0] would get encoded as [0., 1., 0., 1., 0., 0.]. You can get the same result by tweaking the category identifiers so they don’t overlap. For example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beb8c9d30d80afbf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3 + 3)\n",
    "onehot_layer(two_age_categories + [0, 3])  # adds 3 to the second feature"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a293ce29b637af40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this output, the first three columns correspond to the first feature, and the last three correspond to the second feature. This allows the model to distinguish the two features. However, it also increases the number of features fed to the model, and thereby requires more model parameters. It’s hard to know in advance whether a single multi-hot encoding or a per-feature one-hot encoding will work best: it depends on the task, and you may need to test both options.\n",
    "\n",
    "Now you can encode categorical integer features using one-hot or multi-hot encoding. But what about categorical text features? For this, you can use the StringLookup layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f41a91b25586e85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE STRINGLOOKUP LAYER\n",
    "Let’s use the Keras StringLookup layer to one-hot encode a cities feature:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c4051ce0aa01793"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cities = [\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20faff875b6bfe0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We first create a StringLookup layer, then we adapt it to the data: it finds that there are three distinct categories. Then we use the layer to encode a few cities. They are encoded as integers by default. Unknown categories get mapped to 0, as is the case for “Montreal” in this example. The known categories are numbered starting at 1, from the most frequent category to the least frequent.\n",
    "\n",
    "Conveniently, if you set output_mode=\"one_hot\" when creating the StringLookup layer, it will output a one-hot vector for each category, instead of an integer:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15d8dbe34b3eb496"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c500a7de70f32826"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "Keras also includes an IntegerLookup layer that acts much like the StringLookup layer but takes integers as input, rather than strings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d363eb8355fd5dcb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the training set is very large, it may be convenient to adapt the layer to just a random subset of the training set. In this case, the layer’s adapt() method may miss some of the rarer categories. By default, it would then map them all to category 0, making them indistinguishable by the model. To reduce this risk (while still adapting the layer only on a subset of the training set), you can set num_oov_indices to an integer greater than 1. This is the number of out-of-vocabulary (OOV) buckets to use: each unknown category will get mapped pseudorandomly to one of the OOV buckets, using a hash function modulo the number of OOV buckets. This will allow the model to distinguish at least some of the rare categories. For example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1b35a6f27cdbc47"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Foo\"], [\"Bar\"], [\"Baz\"]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4320a8dc5ee3de9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since there are five OOV buckets, the first known category’s ID is now 5 (\"Paris\"). But \"Foo\", \"Bar\", and \"Baz\" are unknown, so they each get mapped to one of the OOV buckets. \"Bar\" gets its own dedicated bucket (with ID 3), but sadly \"Foo\" and \"Baz\" happen to be mapped to the same bucket (with ID 4), so they remain indistinguishable by the model. This is called a hashing collision. The only way to reduce the risk of collision is to increase the number of OOV buckets. However, this will also increase the total number of categories, which will require more RAM and extra model parameters once the categories are one-hot encoded. So, don’t increase that number too much.\n",
    "\n",
    "This idea of mapping categories pseudorandomly to buckets is called the hashing trick. Keras provides a dedicated layer which does just that: the Hashing layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "142e5cf012437982"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE HASHING LAYER\n",
    "For each category, the Keras Hashing layer computes a hash, modulo the number of buckets (or “bins”). The mapping is entirely pseudorandom, but stable across runs and platforms (i.e., the same category will always be mapped to the same integer, as long as the number of bins is unchanged). For example, let’s use the Hashing layer to encode a few cities:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71b0dfd3786e7352"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hashing_layer = tf.keras.layers.Hashing(num_bins=10)\n",
    "hashing_layer([[\"Paris\"], [\"Tokyo\"], [\"Auckland\"], [\"Montreal\"]])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc6df6f0d47a469c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The benefit of this layer is that it does not need to be adapted at all, which may sometimes be useful, especially in an out-of-core setting (when the dataset is too large to fit in memory). However, we once again get a hashing collision: “Tokyo” and “Montreal” are mapped to the same ID, making them indistinguishable by the model. So, it’s usually preferable to stick to the StringLookup layer.\n",
    "\n",
    "Let’s now look at another way to encode categories: trainable embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f499285c2824b72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ENCODING CATEGORICAL FEATURES USING EMBEDDINGS\n",
    "An embedding is a dense representation of some higher-dimensional data, such as a category, or a word in a vocabulary. If there are 50,000 possible categories, then one-hot encoding would produce a 50,000-dimensional sparse vector (i.e., containing mostly zeros). In contrast, an embedding would be a comparatively small dense vector; for example, with just 100 dimensions.\n",
    "\n",
    "In deep learning, embeddings are usually initialized randomly, and they are then trained by gradient descent, along with the other model parameters. For example, the \"NEAR BAY\" category in the California housing dataset could be represented initially by a random vector such as [0.131, 0.890], while the \"NEAR OCEAN\" category might be represented by another random vector such as [0.631, 0.791]. In this example, we use 2D embeddings, but the number of dimensions is a hyperparameter you can tweak.\n",
    "\n",
    "Since these embeddings are trainable, they will gradually improve during training; and as they represent fairly similar categories in this case, gradient descent will certainly end up pushing them closer together, while it will tend to move them away from the \"INLAND\" category’s embedding (see Figure 13-6). Indeed, the better the representation, the easier it will be for the neural network to make accurate predictions, so training tends to make embeddings useful representations of the categories. This is called representation learning (you will see other types of representation learning in Chapter 17)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "194dc88568075025"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 13-6. Embeddings will gradually improve during training\n",
    "![embeddings_will_gradually_improve_during_training](../images/embeddings_will_gradually_improve_during_training.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c881d04d0a0ac3d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "WORD EMBEDDINGS\n",
    "Not only will embeddings generally be useful representations for the task at hand, but quite often these same embeddings can be reused successfully for other tasks. The most common example of this is word embeddings (i.e., embeddings of individual words): when you are working on a natural language processing task, you are often better off reusing pretrained word embeddings than training your own.\n",
    "\n",
    "The idea of using vectors to represent words dates back to the 1960s, and many sophisticated techniques have been used to generate useful vectors, including using neural networks. But things really took off in 2013, when Tomáš Mikolov and other Google researchers published a paper⁠6 describing an efficient technique to learn word embeddings using neural networks, significantly outperforming previous attempts. This allowed them to learn embeddings on a very large corpus of text: they trained a neural network to predict the words near any given word and obtained astounding word embeddings. For example, synonyms had very close embeddings, and semantically related words such as France, Spain, and Italy ended up clustered together.\n",
    "\n",
    "It’s not just about proximity, though: word embeddings were also organized along meaningful axes in the embedding space. Here is a famous example: if you compute King – Man + Woman (adding and subtracting the embedding vectors of these words), then the result will be very close to the embedding of the word Queen (see Figure 13-7). In other words, the word embeddings encode the concept of gender! Similarly, you can compute Madrid – Spain + France, and the result is close to Paris, which seems to show that the notion of capital city was also encoded in the embeddings."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b27387b84bd122d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 13-7. Word embeddings of similar words tend to be close, and some axes seem to encode meaningful concepts\n",
    "![mls3_1307](../images/mls3_1307.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8046ed6ca762216e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, word embeddings sometimes capture our worst biases. For example, although they correctly learn that Man is to King as Woman is to Queen, they also seem to learn that Man is to Doctor as Woman is to Nurse: quite a sexist bias! To be fair, this particular example is probably exaggerated, as was pointed out in a 2019 paper⁠7 by Malvina Nissim et al. Nevertheless, ensuring fairness in deep learning algorithms is an important and active research topic."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35091ee05fba6f85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keras provides an Embedding layer, which wraps an embedding matrix: this matrix has one row per category and one column per embedding dimension. By default, it is initialized randomly. To convert a category ID to an embedding, the Embedding layer just looks up and returns the row that corresponds to that category. That’s all there is to it! For example, let’s initialize an Embedding layer with five rows and 2D embeddings, and use it to encode some categories:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be1a3e0b1801e78d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
    "embedding_layer(np.array([2, 4, 2]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec96d64237cfdffe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, category 2 gets encoded (twice) as the 2D vector [-0.04663396, 0.01846724], while category 4 gets encoded as [-0.02736737, -0.02768031]. Since the layer is not trained yet, these encodings are just random."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70bcc03ad4df03a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "WARNING\n",
    "An Embedding layer is initialized randomly, so it does not make sense to use it outside of a model as a standalone preprocessing layer unless you initialize it with pretrained weights."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e66f799182ad04a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to embed a categorical text attribute, you can simply chain a StringLookup layer and an Embedding layer, like this:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "241ab23b8c988066"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(ocean_prox)\n",
    "lookup_and_embed = tf.keras.Sequential([\n",
    "    str_lookup_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),\n",
    "                              output_dim=2)\n",
    "])\n",
    "\n",
    "lookup_and_embed(np.array([[\"<1H OCEAN\"], [\"ISLAND\"], [\"<1H OCEAN\"]]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46432012ee0810de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the number of rows in the embedding matrix needs to be equal to the vocabulary size: that’s the total number of categories, including the known categories plus the OOV buckets (just one by default). The vocabulary_size() method of the StringLookup class conveniently returns this number."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2b1beaf2412a2a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "In this example we used 2D embeddings, but as a rule of thumb embeddings typically have 10 to 300 dimensions, depending on the task, the vocabulary size, and the size of your training set. You will have to tune this hyperparameter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4892602700a79e5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Putting everything together, we can now create a Keras model that can process a categorical text feature along with regular numerical features and learn an embedding for each category (as well as for each OOV bucket):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f70db153051a9b6e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train_num, X_train_cat, y_train = [...]  # load the training set\n",
    "X_valid_num, X_valid_cat, y_valid = [...]  # and the validation set\n",
    "\n",
    "num_input = tf.keras.layers.Input(shape=[8], name=\"num\")\n",
    "cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"cat\")\n",
    "cat_embeddings = lookup_and_embed(cat_input)\n",
    "encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\n",
    "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
    "model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "history = model.fit((X_train_num, X_train_cat), y_train, epochs=5,\n",
    "                    validation_data=((X_valid_num, X_valid_cat), y_valid))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98026f5ae9a78d14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model takes two inputs: num_input, which contains eight numerical features per instance, plus cat_input, which contains a single categorical text input per instance. The model uses the lookup_and_embed model we created earlier to encode each ocean-proximity category as the corresponding trainable embedding. Next, it concatenates the numerical inputs and the embeddings using the concatenate() function to produce the complete encoded inputs, which are ready to be fed to a neural network. We could add any kind of neural network at this point, but for simplicity we just add a single dense output layer, and then we create the Keras Model with the inputs and output we’ve just defined. Next we compile the model and train it, passing both the numerical and categorical inputs.\n",
    "\n",
    "As you saw in Chapter 10, since the Input layers are named \"num\" and \"cat\", we could also have passed the training data to the fit() method using a dictionary instead of a tuple: {\"num\": X_train_num, \"cat\": X_train_cat}. Alternatively, we could have passed a tf.data.Dataset containing batches, each represented as ((X_batch_num, X_batch_cat), y_batch) or as ({\"num\": X_batch_num, \"cat\": X_batch_cat}, y_batch). And of course the same goes for the validation data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57758a46da893d1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "One-hot encoding followed by a Dense layer (with no activation function and no biases) is equivalent to an Embedding layer. However, the Embedding layer uses way fewer computations as it avoids many multiplications by zero—the performance difference becomes clear when the size of the embedding matrix grows. The Dense layer’s weight matrix plays the role of the embedding matrix. For example, using one-hot vectors of size 20 and a Dense layer with 10 units is equivalent to using an Embedding layer with input_dim=20 and output_dim=10. As a result, it would be wasteful to use more embedding dimensions than the number of units in the layer that follows the Embedding layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae911cb5da29176b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "OK, now that you have learned how to encode categorical features, it’s time to turn our attention to text preprocessing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d713efc5d8f0eda2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEXT PREPROCESSING\n",
    "Keras provides a TextVectorization layer for basic text preprocessing. Much like the StringLookup layer, you must either pass it a vocabulary upon creation, or let it learn the vocabulary from some training data using the adapt() method. Let’s look at an example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec6e682a41d0755"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = [\"To be\", \"!(to be)\", \"That's the question\", \"Be, be, be.\"]\n",
    "text_vec_layer = tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6d1211459a0e5d2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two sentences “Be good!” and “Question: be or be?” were encoded as [2, 1, 0, 0] and [6, 2, 1, 2], respectively. The vocabulary was learned from the four sentences in the training data: “be” = 2, “to” = 3, etc. To construct the vocabulary, the adapt() method first converted the training sentences to lowercase and removed punctuation, which is why “Be”, “be”, and “be?” are all encoded as “be” = 2. Next, the sentences were split on whitespace, and the resulting words were sorted by descending frequency, producing the final vocabulary. When encoding sentences, unknown words get encoded as 1s. Lastly, since the first sentence is shorter than the second, it was padded with 0s."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6360a18020e2e84b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "The TextVectorization layer has many options. For example, you can preserve the case and punctuation if you want, by setting standardize=None, or you can pass any standardization function you please as the standardize argument. You can prevent splitting by setting split=None, or you can pass your own splitting function instead. You can set the output_sequence_length argument to ensure that the output sequences all get cropped or padded to the desired length, or you can set ragged=True to get a ragged tensor instead of a regular tensor. Please check out the documentation for more options."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d372c6895a47e155"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The word IDs must be encoded, typically using an Embedding layer: we will do this in Chapter 16. Alternatively, you can set the TextVectorization layer’s output_mode argument to \"multi_hot\" or \"count\" to get the corresponding encodings. However, simply counting words is usually not ideal: words like “to” and “the” are so frequent that they hardly matter at all, whereas, rarer words such as “basketball” are much more informative. So, rather than setting output_mode to \"multi_hot\" or \"count\", it is usually preferable to set it to \"tf_idf\", which stands for term-frequency × inverse-document-frequency (TF-IDF). This is similar to the count encoding, but words that occur frequently in the training data are downweighted, and conversely, rare words are upweighted. For example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5affd262f54c775"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
    "text_vec_layer.adapt(train_data)\n",
    "text_vec_layer([\"Be good!\", \"Question: be or be?\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af047d55504935bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are many TF-IDF variants, but the way the TextVectorization layer implements it is by multiplying each word count by a weight equal to log(1 + d / (f + 1)), where d is the total number of sentences (a.k.a., documents) in the training data and f counts how many of these training sentences contain the given word. For example, in this case there are d = 4 sentences in the training data, and the word “be” appears in f = 3 of these. Since the word “be” occurs twice in the sentence “Question: be or be?”, it gets encoded as 2 × log(1 + 4 / (1 + 3)) ≈ 1.3862944. The word “question” only appears once, but since it is a less common word, its encoding is almost as high: 1 × log(1 + 4 / (1 + 1)) ≈ 1.0986123. Note that the average weight is used for unknown words.\n",
    "\n",
    "This approach to text encoding is straightforward to use and it can give fairly good results for basic natural language processing tasks, but it has several important limitations: it only works with languages that separate words with spaces, it doesn’t distinguish between homonyms (e.g., “to bear” versus “teddy bear”), it gives no hint to your model that words like “evolution” and “evolutionary” are related, etc. And if you use multi-hot, count, or TF-IDF encoding, then the order of the words is lost. So what are the other options?\n",
    "\n",
    "One option is to use the TensorFlow Text library, which provides more advanced text preprocessing features than the TextVectorization layer. For example, it includes several subword tokenizers capable of splitting the text into tokens smaller than words, which makes it possible for the model to more easily detect that “evolution” and “evolutionary” have something in common (more on subword tokenization in Chapter 16).\n",
    "\n",
    "Yet another option is to use pretrained language model components. Let’s look at this now."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b75a65e698f22b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# USING PRETRAINED LANGUAGE MODEL COMPONENTS\n",
    "The TensorFlow Hub library makes it easy to reuse pretrained model components in your own models, for text, image, audio, and more. These model components are called modules. Simply browse the TF Hub repository, find the one you need, and copy the code example into your project, and the module will be automatically downloaded and bundled into a Keras layer that you can directly include in your model. Modules typically contain both preprocessing code and pretrained weights, and they generally require no extra training (but of course, the rest of your model will certainly require training).\n",
    "\n",
    "For example, some powerful pretrained language models are available. The most powerful are quite large (several gigabytes), so for a quick example let’s use the nnlm-en-dim50 module, version 2, which is a fairly basic module that takes raw text as input and outputs 50-dimensional sentence embeddings. We’ll import TensorFlow Hub and use it to load the module, then use that module to encode two sentences to vectors:8"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35fb0e81118d1564"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
    "sentence_embeddings.numpy().round(2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb40fd5daa28109a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The hub.KerasLayer layer downloads the module from the given URL. This particular module is a sentence encoder: it takes strings as input and encodes each one as a single vector (in this case, a 50-dimensional vector). Internally, it parses the string (splitting words on spaces) and embeds each word using an embedding matrix that was pretrained on a huge corpus: the Google News 7B corpus (seven billion words long!). Then it computes the mean of all the word embeddings, and the result is the sentence embedding.⁠9\n",
    "\n",
    "You just need to include this hub_layer in your model, and you’re ready to go. Note that this particular language model was trained on the English language, but many other languages are available, as well as multilingual models.\n",
    "\n",
    "Last but not least, the excellent open source Transformers library by Hugging Face also makes it easy to include powerful language model components inside your own models. You can browse the Hugging Face Hub, choose the model you want, and use the provided code examples to get started. It used to contain only language models, but it has now expanded to include image models and more.\n",
    "\n",
    "We will come back to natural language processing in more depth in Chapter 16. Let’s now look at Keras’s image preprocessing layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63638eebe2c1a734"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IMAGE PREPROCESSING LAYERS\n",
    "The Keras preprocessing API includes three image preprocessing layers:\n",
    "\n",
    "- tf.keras.layers.Resizing resizes the input images to the desired size. For example, Resizing(height=100, width=200) resizes each image to 100 × 200, possibly distorting the image. If you set crop_to_aspect_ratio=True, then the image will be cropped to the target image ratio, to avoid distortion.\n",
    "\n",
    "- tf.keras.layers.Rescaling rescales the pixel values. For example, Rescal⁠ing​(scale=2/255, offset=-1) scales the values from 0 → 255 to –1 → 1.\n",
    "\n",
    "- tf.keras.layers.CenterCrop crops the image, keeping only a center patch of the desired height and width.\n",
    "\n",
    "For example, let’s load a couple of sample images and center-crop them. For this, we will use Scikit-Learn’s load_sample_images() function; this loads two color images, one of a Chinese temple and the other of a flower (this requires the Pillow library, which should already be installed if you are using Colab or if you followed the installation instructions):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd2afa0fa1c7a9a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_images\n",
    "\n",
    "images = load_sample_images()[\"images\"]\n",
    "crop_image_layer = tf.keras.layers.CenterCrop(height=100, width=100)\n",
    "cropped_images = crop_image_layer(images)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef241b6c079f7d8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keras also includes several layers for data augmentation, such as RandomCrop, RandomFlip, RandomTranslation, RandomRotation, RandomZoom, RandomHeight, RandomWidth, and RandomContrast. These layers are only active during training, and they randomly apply some transformation to the input images (their names are self-explanatory). Data augmentation will artificially increase the size of the training set, which often leads to improved performance, as long as the transformed images look like realistic (nonaugmented) images. We’ll cover image processing more closely in the next chapter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0d8a655df338a4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE\n",
    "Under the hood, the Keras preprocessing layers are based on TensorFlow’s low-level API. For example, the Normalization layer uses tf.nn.moments() to compute both the mean and variance, the Discretization layer uses tf.raw_ops.Bucketize(), CategoricalEncoding uses tf.math.bincount(), IntegerLookup and StringLookup use the tf.lookup package, Hashing and TextVectorization use several ops from the tf.strings package, Embedding uses tf.nn.embedding_lookup(), and the image preprocessing layers use the ops from the tf.image package. If the Keras preprocessing API isn’t sufficient for your needs, you may occasionally need to use TensorFlow’s low-level API directly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33bdef5643a40e2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let’s look at another way to load data easily and efficiently in TensorFlow."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afb1667a847cc5be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# THE TENSORFLOW DATASETS PROJECT\n",
    "The TensorFlow Datasets (TFDS) project makes it very easy to load common datasets, from small ones like MNIST or Fashion MNIST to huge datasets like ImageNet (you will need quite a bit of disk space!). The list includes image datasets, text datasets (including translation datasets), audio and video datasets, time series, and much more. You can visit https://homl.info/tfds to view the full list, along with a description of each dataset. You can also check out Know Your Data, which is a tool to explore and understand many of the datasets provided by TFDS.\n",
    "\n",
    "TFDS is not bundled with TensorFlow, but if you are running on Colab or if you followed the installation instructions at https://homl.info/install, then it’s already installed. You can then import tensorflow_datasets, usually as tfds, then call the tfds.load() function, which will download the data you want (unless it was already downloaded earlier) and return the data as a dictionary of datasets (typically one for training and one for testing, but this depends on the dataset you choose). For example, let’s download MNIST:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81deb2c43f81a07a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23e2133a71efe4d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can then apply any transformation you want (typically shuffling, batching, and prefetching), and you’re ready to train your model. Here is a simple example:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28a0ba858bbf2e3e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for batch in mnist_train.shuffle(10_000, seed=42).batch(32).prefetch(1):\n",
    "    images = batch[\"image\"]\n",
    "    labels = batch[\"label\"]\n",
    "    # [...] do something with the images and labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2040fef0bf61a670"
  },
  {
   "cell_type": "markdown",
   "source": [
    "TIP\n",
    "The load() function can shuffle the files it downloads: just set shuffle_files=True. However, this may be insufficient, so it’s best to shuffle the training data some more."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a5fc9c572a2a62e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that each item in the dataset is a dictionary containing both the features and the labels. But Keras expects each item to be a tuple containing two elements (again, the features and the labels). You could transform the dataset using the map() method, like this:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35014d4c333c736f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.shuffle(buffer_size=10_000, seed=42).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
    "mnist_train = mnist_train.prefetch(1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1de598caffee7cdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "But it’s simpler to ask the load() function to do this for you by setting as_supervised=True (obviously this works only for labeled datasets).\n",
    "\n",
    "Lastly, TFDS provides a convenient way to split the data using the split argument. For example, if you want to use the first 90% of the training set for training, the remaining 10% for validation, and the whole test set for testing, then you can set split=[\"train[:90%]\", \"train[90%:]\", \"test\"]. The load() function will return all three sets. Here is a complete example, loading and splitting the MNIST dataset using TFDS, then using these sets to train and evaluate a simple Keras model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc91fa516f8382dc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = tfds.load(\n",
    "    name=\"mnist\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "train_set = train_set.shuffle(buffer_size=10_000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = valid_set.batch(32).cache()\n",
    "test_set = test_set.batch(32).cache()\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_set)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ee8d7a5fdbfd49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congratulations, you’ve reached the end of this quite technical chapter! You may feel that it is a bit far from the abstract beauty of neural networks, but the fact is deep learning often involves large amounts of data, and knowing how to load, parse, and preprocess it efficiently is a crucial skill to have. In the next chapter, we will look at convolutional neural networks, which are among the most successful neural net architectures for image processing and many other applications."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466661c76bacdd9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EXTRA MATERIALS / EXERCISES"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "379cd5d5e4fa9101"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
