{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-25T08:05:16.337682Z",
     "start_time": "2024-08-25T08:05:16.333670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: The math formulas vs the authors implementation of support vector machines is\n",
    "#  not clear. If interested, spend some more time on this in next iteration.\n",
    "\n",
    "# TODO: In some cases, once the support vector is computed here in my code, some datapoints are\n",
    "#  not within the margin. It might be that there is some little difference between\n",
    "#  how the support vector is computed and how the top and lower boundaries are\n",
    "#  computed in my plotting function. It usually happens when the two classes merge \n",
    "#  a lot together in soft margin classification. This should be investigated. \n",
    "#    --> I checked that with LLMs and seems that this behavior is normal, when there are some outliers."
   ],
   "id": "cf52375625c674f0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Decision function\n",
    "The decision function for a support vector classifier (SVC) can be derived from the linear equation that defines the \n",
    "hyperplane separating the two classes in the feature space. For a linear SVC, the decision function is given by:\n",
    "\n",
    "$f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $\\mathbf{w}$ is the weight vector (coefficients) of the hyperplane.\n",
    "- $\\mathbf{x}$ is the input feature vector.\n",
    "- $b$ is the bias term (intercept).\n",
    "\n",
    "The decision function outputs a real-valued number, which is the distance of the point $\\mathbf{x}$ from the hyperplane. The sign of the decision function determines the class label:\n",
    "\n",
    "- If $f(\\mathbf{x}) > 0$, the point is classified as belonging to the positive class.\n",
    "- If $f(\\mathbf{x}) < 0$, the point is classified as belonging to the negative class.\n",
    "- If $f(\\mathbf{x}) = 0$, the point lies exactly on the decision boundary (hyperplane)."
   ],
   "id": "d794e9476d91f8b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Label transformation\n",
    "In Support Vector Classifiers (SVC), the labels of the classes are typically transformed to -1 and 1 to simplify the optimization problem. This transformation is particularly useful because it allows the classifier to express the decision boundary in a standardized way, leveraging the mathematical properties of these binary labels.\n",
    "\n",
    "### Transformation Formula\n",
    "\n",
    "Assume you have a dataset with binary class labels, typically denoted as 0 and 1. To transform these labels into -1 and 1 for use in a Support Vector Classifier, you can use the following formula:\n",
    "\n",
    "$\n",
    "y' = 2y - 1\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $y$ is the original label, either 0 or 1.\n",
    "- $y'$ is the transformed label, either -1 or 1.\n",
    "\n",
    "### Example\n",
    "\n",
    "If $y = 0$:\n",
    "$\n",
    "y' = 2(0) - 1 = -1\n",
    "$\n",
    "\n",
    "If $y = 1$:\n",
    "$\n",
    "y' = 2(1) - 1 = 1\n",
    "$\n",
    "\n",
    "### Why This Transformation?\n",
    "\n",
    "The transformation to -1 and 1 is advantageous in the context of SVC because:\n",
    "\n",
    "1. **Simplifies the Optimization Problem**: The optimization problem in SVMs involves maximizing the margin between the classes. With labels -1 and 1, the margin can be directly related to the distance from the decision boundary, which is expressed as $f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b$. This simplifies the constraints to $y' \\cdot f(\\mathbf{x}) \\geq 1$.\n",
    "\n",
    "2. **Mathematical Properties**: The transformation leverages the algebraic properties of these labels in dot products and other operations during the optimization process, leading to more straightforward and efficient computations.\n",
    "\n",
    "This transformation is standard practice in the implementation of binary SVMs. For multi-class SVMs, similar principles apply, but the transformation and decision process involve handling multiple binary classifiers."
   ],
   "id": "4b311d90f14bd8bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Margin\n",
    "In classification task, you compute support vectors which are basically a datapoints within the margin. Your aim is to\n",
    "have the widest possible margin and at the same time have as few support vectors as possible. \n",
    "The computation of parameters is done only using the support vectors. Data points that are not support vectors are not used in the computation.\n",
    "\n",
    "In soft margin classification some datapoints are allowed to be within the margin and misclassified.\n",
    "  \n",
    "In hard margin classification the aim is to have only datapoints exactly on the edges of the margin.\n",
    "Only linearly separable data can be used in hard margin classification.\n",
    "\n",
    "In regression task the objective is the opposite. You want to fit as many datapoints as possible into the margin. \n",
    "This time however, the datapoints that are not within the margin are support vectors. "
   ],
   "id": "e157e5f5546449"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance\n",
    "SVM scale poorly with huge datasets. It makes sense to use them for small to medium datasets."
   ],
   "id": "ced34d26f5b1d76f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
