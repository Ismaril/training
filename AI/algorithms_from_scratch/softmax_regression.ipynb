{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SOFTMAX REGRESSION\n",
    "\n",
    "Softmax regression, also known as multinomial logistic regression, is a generalization of logistic regression to the case where we want to handle multiple classes. It's widely used for multiclass classification problems where each instance can belong to one of several classes. The goal of softmax regression is to estimate probabilities of each class that sum up to 1, making it a natural extension for problems beyond binary classification.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "286aee425dcbc8a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In Softmax Regression (also known as Multinomial Logistic Regression), the parameters (denoted as \\(\\theta\\)) are updated using gradient descent or some other optimization technique. The update rule is derived from the cross-entropy loss function, which is used to measure the difference between the predicted probabilities and the actual labels.\n",
    "\n",
    "### Softmax Function\n",
    "First, the softmax function converts the linear combination of features into a probability distribution over classes:\n",
    "\n",
    "$\n",
    "P(y = j \\mid x; \\theta) = \\frac{\\exp(\\theta_j^T x)}{\\sum_{k=1}^{K} \\exp(\\theta_k^T x)}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $P(y = j \\mid x; \\theta)$ is the predicted probability that the input $x$ belongs to class $j$.\n",
    "- $\\theta_j$ is the parameter vector for class $j$.\n",
    "- $K$ is the total number of classes.\n",
    "\n",
    "### Cross-Entropy Loss Function\n",
    "$J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} log(\\hat{p}_k^{(i)})$\n",
    "\n",
    "### Gradient Descent Update Rule - Vectorized Form\n",
    "In vectorized form, for all classes $K$, the update rule is:\n",
    "\n",
    "$\n",
    "\\Theta := \\Theta - \\alpha X^T (\\mathbf{Y} - \\mathbf{P})\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\Theta$ is a matrix of parameters where each column corresponds to the parameter vector $\\theta_j$ for each class $j$.\n",
    "- $X$ is the matrix of input features.\n",
    "- $\\mathbf{Y}$ is a binary matrix indicating the actual classes for each example.\n",
    "- $\\mathbf{P}$ is the matrix of predicted probabilities for each class and each example.\n",
    "\n",
    "This formula updates the parameter matrix $\\Theta$ in the direction that reduces the cross-entropy loss, adjusting the model to improve its predictions."
   ],
   "id": "9c26add103e621d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SOFTMAX REGRESSION FROM SCRATCH"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e699d57e0c6fea11"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from my_extensions import MyRegressionBase\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# IMPORT MNIST DATA\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "X, y = np.array(mnist[\"data\"]), np.array(mnist[\"target\"])\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:06.176075Z",
     "start_time": "2024-08-13T15:45:02.438071Z"
    }
   },
   "id": "f47d684403ec0348",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:06.180638Z",
     "start_time": "2024-08-13T15:45:06.177080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X.shape"
   ],
   "id": "667dde56d571070f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:06.185172Z",
     "start_time": "2024-08-13T15:45:06.181644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y.shape"
   ],
   "id": "b9200d39db397035",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:06.190231Z",
     "start_time": "2024-08-13T15:45:06.186177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y[:10]"
   ],
   "id": "9c987f98ceb10101",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5', '0', '4', '1', '9', '2', '1', '3', '1', '4'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "source": [
    "# SPLIT DATA INTO TRAINING AND TESTING SETS\n",
    "\n",
    "# Add 1s column to the X matrix to account for the bias term\n",
    "X = np.c_[np.ones((len(X), 1)), X]\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:06.364374Z",
     "start_time": "2024-08-13T15:45:06.190231Z"
    }
   },
   "id": "94746993d3b6eb3d",
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "source": [
    "# NORMALIZE DATA\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:06.445194Z",
     "start_time": "2024-08-13T15:45:06.364374Z"
    }
   },
   "id": "422b328eb7afa2dd",
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# SOFTMAX FUNCTION\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    Returns the values of the logits vector after applying the softmax function.\n",
    "    Axis 1 is used to sum each row of the matrix and return new array. \n",
    "    Keep dims is used to keep the dimensions of the original array.\n",
    "    [[1 2 3] [4 5 6]] -> [[6] [15]] \n",
    "    \n",
    "    :param logits: Features, computed as linear model. Size: (nr_observations, n_classes)\n",
    "    :return: np.array of probabilities where each row sums to 1. Size: (nr_observations, n_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    return np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# CROSS-ENTROPY LOSS FUNCTION               \n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss.\n",
    "\n",
    "    Parameters:\n",
    "    - y: np.array, one-hot encoded true labels. Shape: (nr_observations, n_classes)\n",
    "    - y_hat: np.array, predicted probabilities. Shape: (nr_observations, n_classes)\n",
    "\n",
    "    Returns:\n",
    "    - loss: float, the cross-entropy loss averaged over all samples.\n",
    "    \"\"\"\n",
    "    # Small constant to ensure numerical stability and avoid log(0)\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = -np.sum(y * np.log(y_hat + epsilon))\n",
    "\n",
    "    # Average over all samples\n",
    "    loss /= y.shape[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# CREATE SOFTMAX REGRESSION MODEL FROM SCRATCH\n",
    "# Inherit from the Extension class to use its methods for visualization and evaluation\n",
    "class MySoftmaxRegression(MyRegressionBase):\n",
    "    def __init__(self, nr_of_features, nr_of_classes, learning_rate=0.1, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Softmax regression model.\n",
    "        \n",
    "        It relies on linear regression model. All the computations are the same as in the Linear Regression model. \n",
    "        There are added few more things to it.\n",
    "        In this case we have number of thetas as number of features (as usual) but also the same for each other class. \n",
    "        This in practise means that if you have in your dataPoints 3 features, and result could be one of 5 classes \n",
    "        then you will have 5 thetas x 3 features = 15 thetas, therefore thetas in a matrix shape (3, 5).\n",
    "        \n",
    "        In case of example in this notebook, we got 784 features (28x28 pixels) and 10 classes (digits from 0 to 9).\n",
    "        The weight matrix will have shape (784, 10):\n",
    "          class 0, class 1, ..., class 9\n",
    "        [ w0_0   , w0_1   , ..., w0_9   ]\n",
    "        [ w1_0   , w1_1   , ..., w1_9   ]\n",
    "        ...\n",
    "        [ w784_0 , w784_1 , ..., w784_9 ]\n",
    "        \n",
    "        The way I understand it is that we are basically preparing weights for all pixels and for each class.\n",
    "        Therefore, if we then take this matrix and dot product it with features (using linear regression), we get a new \n",
    "        matrix where in each column we have scores/results corresponding to each class of a given datapoint. This means \n",
    "        that for example column 0 in the weight matrix (with 784 rows) will be calculated together with the feature \n",
    "        (all 784 pixels) and the resulting number/score is then the result for class 0 of a given datapoint. The same \n",
    "        goes for all other classes. \n",
    "        I noticed that the biggest number in each row is then latter also the winning class of each datapoint once the \n",
    "        softmax function is applied to it. (to the whole matrix)\n",
    "        \n",
    "        :param nr_of_features: Number of features per sample/observation \n",
    "        :param nr_of_classes: Number of possible classes per sample/observation\n",
    "        :param learning_rate: How fast the model should learn\n",
    "        :param n_iterations: How many times the model should learn\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.nr_features = nr_of_features\n",
    "        self.nr_of_classes = nr_of_classes\n",
    "        self.thetas = []\n",
    "        \n",
    "        # just for visualization ----------------------\n",
    "        self.losses = []\n",
    "        self.y_pred_linear = None\n",
    "        self.y_pred_softmax = None\n",
    "        self.gradients = None\n",
    "        # ---------------------------------------------\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the data X and y. Meaning let the model learn the parameters.\n",
    "        :param X: Features\n",
    "        :param y: Labels\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        nr_observations = len(X)\n",
    "        \n",
    "        # one-hot encode the labels (example in a cell below) Shape: (nr_observations, n_classes) (60_000, 10)\n",
    "        Y = np.eye(self.nr_of_classes)[y.astype(int)]\n",
    "\n",
    "        # initialize the parameters with random values Shape: (nr_features, n_classes) (785, 10)\n",
    "        self.thetas = np.random.randn(self.nr_features, self.nr_of_classes)\n",
    "\n",
    "        # gradient descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # compute linear model (linear predictions) Shape: (nr_observations, n_classes) (60_000, 10)\n",
    "            y_pred_linear = np.dot(X, self.thetas)\n",
    "\n",
    "            # apply softmax function to get probabilities Shape: (nr_observations, n_classes) (60_000, 10)\n",
    "            y_pred_softmax = softmax(y_pred_linear)\n",
    "             \n",
    "            # compute the gradients Shape: (nr_features, n_classes) (785, 10)\n",
    "            gradients = 2 / nr_observations * np.dot(X.T, (y_pred_softmax - Y))\n",
    "\n",
    "            # update the parameters Shape: (nr_features, n_classes) (785, 10)\n",
    "            self.thetas = self.thetas - self.learning_rate * gradients\n",
    "            \n",
    "            # -----------------------------------------------------------------------\n",
    "            # (only for visualization, not used in the optimization process)\n",
    "            # compute the cost \n",
    "            loss = cross_entropy_loss(Y, y_pred_softmax)\n",
    "            self.losses.append(loss)\n",
    "            self.y_pred_linear = y_pred_linear\n",
    "            self.y_pred_softmax = y_pred_softmax\n",
    "            self.gradients = gradients\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the provided data\n",
    "        :param X: Features in the shape (nr_observations, nr_features) (any, 785)\n",
    "        :return: Predicted class labels\n",
    "        \"\"\"\n",
    "        linear_model = np.dot(X, self.thetas)\n",
    "        return softmax(linear_model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:06.453397Z",
     "start_time": "2024-08-13T15:45:06.446199Z"
    }
   },
   "id": "217dc729d016274d",
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "source": [
    "# 4 - we specify that we are working with 4 classes\n",
    "# [0, 1, 2, 3, 3, 2, 1, 0] - Array we want to one-hot encode\n",
    "Y = np.eye(4)[[0, 1, 2, 3, 3, 2, 1, 0]]\n",
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:06.458551Z",
     "start_time": "2024-08-13T15:45:06.454467Z"
    }
   },
   "id": "8a8a90212465eb95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "source": [
    "# TRAIN THE MODEL (Takes 10 minutes to run. If you want to make it faster, then implement mini-batch gradient descent or SGD, instead of batch gradient descent)\n",
    "\n",
    "model = MySoftmaxRegression(nr_of_features=X_train.shape[1], # 784, because 28x28 pixels + 1 for bias\n",
    "                            nr_of_classes=len(np.unique(y_train)), # 10, because 10 digits\n",
    "                            learning_rate=0.1,\n",
    "                            n_iterations=10_000)\n",
    "model.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T15:45:59.305341Z",
     "start_time": "2024-08-13T15:45:06.459557Z"
    }
   },
   "id": "5e31262f0628cdfc",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[91], line 7\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# TRAIN THE MODEL (Takes 10 minutes to run. If you want to make it faster, then implement mini-batch gradient descent or SGD, instead of batch gradient descent)\u001B[39;00m\n\u001B[0;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m MySoftmaxRegression(nr_of_features\u001B[38;5;241m=\u001B[39mX_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;66;03m# 784, because 28x28 pixels + 1 for bias\u001B[39;00m\n\u001B[0;32m      4\u001B[0m                             nr_of_classes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(y_train)), \u001B[38;5;66;03m# 10, because 10 digits\u001B[39;00m\n\u001B[0;32m      5\u001B[0m                             learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,\n\u001B[0;32m      6\u001B[0m                             n_iterations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10_000\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[89], line 114\u001B[0m, in \u001B[0;36mMySoftmaxRegression.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    111\u001B[0m y_pred_softmax \u001B[38;5;241m=\u001B[39m softmax(y_pred_linear)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;66;03m# compute the gradients Shape: (nr_features, n_classes) (785, 10)\u001B[39;00m\n\u001B[1;32m--> 114\u001B[0m gradients \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m/\u001B[39m nr_observations \u001B[38;5;241m*\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43my_pred_softmax\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;66;03m# update the parameters Shape: (nr_features, n_classes) (785, 10)\u001B[39;00m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthetas \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthetas \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m*\u001B[39m gradients\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "source": [
    "# show the parameters\n",
    "print(\"Best parameters:\")\n",
    "model.show_parameters()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9bf2abe4a51c06a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.y_pred_linear"
   ],
   "id": "8a969ef58232ab78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rounded_softmax = model.y_pred_softmax.round(4)\n",
    "rounded_softmax"
   ],
   "id": "21e7a505d9e1aee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# there are places where are clean 0, I guess that might be due to the fact that the images have areas where the \n",
    "# pixels do not hold any information (the number which we predict is not there), so it just probably sets it to 0.\n",
    "rounded_gradients = model.gradients.round(6)\n",
    "rounded_gradients"
   ],
   "id": "23c5854821340dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# VISUALIZE THE MODELS LOSS CURVE\n",
    "\n",
    "model.show_loss_curve()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc904af8d83863f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# EVALUATE THE MODEL\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(np.argmax(y_pred, axis=1) == y_test.astype(int))\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f36af1331adb2d5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In those few cells below I demonstrate how you can take only one vector containing thetas/parameters of a given class and use only binary classificator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24d0db9d4e4c3e"
  },
  {
   "cell_type": "code",
   "source": [
    "# take a sample from the training set, which is number 0\n",
    "X_sample_0 = X_train[108]\n",
    "\n",
    "# visualise the sample with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# [1:] to remove the bias term\n",
    "plot_digit(X_sample_0[1:])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdd31d76c669a89b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"\n",
    "    Returns the values of the t vector after applying the sigmoid function.\n",
    "    \n",
    "    :param t: Feature vector. \n",
    "    :return: np.array\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def predict(X, thetas):\n",
    "    \"\"\"\n",
    "    Just a helper function to predict sigmoid class probabilities\n",
    "    \"\"\"\n",
    "    linear_model = np.dot(X, thetas)\n",
    "    y_predicted = sigmoid(linear_model)\n",
    "    return y_predicted\n",
    "\n",
    "for i, softmax_probability in enumerate(model.predict([X_sample_0])[0]):\n",
    "    sigmoid_probability = predict(X_sample_0, model.thetas[:, i])\n",
    "    print(f\"Class:{i}\", f\"Sigmoid:{sigmoid_probability:.5f}\", f\"Softmax:{softmax_probability:.5f}\", sep=\", \")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c538533164ee8acb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
