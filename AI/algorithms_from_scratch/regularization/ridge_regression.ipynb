{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RIDGE REGRESSION COST FUNCTION\n",
    "\n",
    "Ridge Regression, also known as Tikhonov regularization, is a type of linear regression that includes a regularization term. The purpose of this regularization term is to penalize large coefficients in the model, which can help prevent overfitting and improve the model's generalization to new data. The cost function for Ridge Regression combines the Residual Sum of Squares (RSS) with a penalty on the size of the coefficients."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d5a5e040b1414e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The Ridge Regression cost function is given by:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{m} \\theta_j^2$\n",
    "\n",
    "where:\n",
    "- $J(\\theta)$ is the cost function to be minimized.\n",
    "- $n$ is the number of observations.\n",
    "- $y_i$ is the actual value for the $i^{th}$ observation.\n",
    "- $\\hat{y}_i$ is the predicted value for the $i^{th}$ observation, which is calculated as $\\hat{y}_i = \\theta^T x_i$ where $\\theta$ is the coefficient vector and $x_i$ is the feature vector for the $i^{th}$ observation.\n",
    "- $\\lambda$ is the regularization parameter, a hyperparameter that controls the amount of shrinkage: the larger the value of $\\lambda$, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity.\n",
    "- $\\theta_j$ are the model coefficients, and $m$ is the number of features (excluding the intercept).\n",
    "- The first term is the RSS divided by $2n$, which normalizes the RSS by the number of observations.\n",
    "- The second term is the regularization term, where $\\lambda \\sum_{j=1}^{m} \\theta_j^2$ adds a penalty for large coefficients."
   ],
   "id": "923e8e82546807c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How to update thetas:\\\n",
    "$\\nabla_{\\theta} J(\\theta) = \\frac{1}{m} X^T (X\\theta - y) + \\lambda \\theta$\n",
    "    \n",
    "Where:\n",
    "- The first part is classic gradient descent for linear regression\n",
    "- $\\lambda \\theta$ is the regularization term where we multiply lambda (sometimes they use alpha) with all thetas except the bias\n",
    "\n",
    "Full picture of updating thetas:\\\n",
    "$\\theta := \\theta - \\alpha \\left( \\frac{1}{m} X^T (X\\theta - y) + \\lambda \\theta \\right)$\n",
    "    "
   ],
   "id": "f48c18020dfdd4be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "For info, sometimes you might se 1/2. Both formulations are mathematically valid and will lead to the same set of coefficients after optimization, although the effective value of $\\lambda$ may need to be adjusted between the two formulations to achieve the same level of regularization. The choice of including the 1/2 factor is mostly a matter of convenience for simplifying calculus operations and does not impact the goal of regularization, which is to penalize large coefficients to improve model generalization.\n",
    "\n",
    "$\\lambda \\sum_{j=1}^{m} \\theta_j^2$\\\n",
    "$\\frac{1}{2} \\lambda \\sum_{j=1}^{m} \\theta_j^2$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "927ea91e48d51d28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CLOSED FORM SOLUTION\n",
    "\n",
    "Ridge regression extends linear regression by adding a regularization term to the cost function, which penalizes large coefficients to prevent overfitting. The closed-form solution for Ridge regression, also known as the Ridge regression normal equation, is given by:\n",
    "\n",
    "$\\hat{\\theta} = ((X^T \\cdot X)+ \\lambda \\cdot I)^{-1} \\cdot (X^T \\cdot y) $\n",
    "\n",
    "where:\n",
    "- $\\hat{\\theta}$ - is the vector of coefficients, meaning estimated model parameters that minimize the cost function.\n",
    "- $X$ - is the matrix of feature values, with each row representing an observation and each column a feature. An additional column of ones is typically added to $X$ to include the intercept term.\n",
    "- $y$ - is the target/labels vector.\n",
    "- $\\lambda$ - is the regularization parameter, a non-negative value that controls the strength of the regularization. Larger values of $\\lambda$ impose a greater penalty on the size of the coefficients.\n",
    "- $I$ - is the identity matrix, with the same number of rows and columns as the number of features (including the intercept). The first diagonal element is often set to 0 to exclude the intercept from regularization.\n",
    "- $X^T$ - is the transpose of $X$.\n",
    "- $(X^T X + \\lambda I)^{-1}$ - is the inverse of $X^T X + \\lambda I$.\n",
    "- $X^T y$ - is the matrix multiplication of $X^T$ and $y$.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1a161d3363212c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# You can find practical example in the polynomial_regression.ipynb notebook",
   "id": "915dc53a6bf86de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
