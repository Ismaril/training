{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# LASSO REGRESSION",
   "metadata": {
    "collapsed": false
   },
   "id": "ec2cfc86e1d93f3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Lasso Regression Loss Function\n",
    "The Lasso (Least Absolute Shrinkage and Selection Operator) regression loss function is given by:\n",
    "\n",
    "$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
    "$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of training examples.\n",
    "- $h_{\\theta}(x) = \\theta^T x$ is the hypothesis or predicted value.\n",
    "- $y^{(i)}$ is the actual target value.\n",
    "- $\\lambda$ is the regularization parameter.\n",
    "- $\\theta_j$ represents the model parameters.\n",
    "\n",
    "### Gradient of the Loss Function\n",
    "To perform gradient descent, we need to compute the subgradient of the loss function with respect to $\\theta$ because the $L1$ term is not differentiable at $\\theta_j = 0$. The subgradient for the absolute value function $ |\\theta_j| $ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} =\n",
    "\\begin{cases}\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)} + \\lambda & \\text{if } \\theta_j > 0 \\\\\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)} - \\lambda & \\text{if } \\theta_j < 0 \\\\\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} \\left( h_{\\theta}(x^{(i)}) - y^{(i)} \\right) x_j^{(i)} & \\text{if } \\theta_j = 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "This can be expressed in vectorized form (ignoring the piecewise nature for simplicity) as:\n",
    "\n",
    "$\n",
    "\\nabla_{\\theta} J(\\theta) = \\frac{1}{m} X^T (X\\theta - y) + \\lambda \\cdot \\text{sign}(\\theta)\n",
    "$\n",
    "\n",
    "Where $ \\text{sign}(\\theta) $ is the sign function, which returns $-1$, $0$, or $1$ depending on the value of $\\theta_j$.\n",
    "\n",
    "### Gradient Descent Update Rule\n",
    "The update rule for $\\theta$ using gradient descent is:\n",
    "\n",
    "$\n",
    "\\theta := \\theta - \\alpha \\nabla_{\\theta} J(\\theta)\n",
    "$\n",
    "\n",
    "Substituting the subgradient, we get:\n",
    "\n",
    "$\n",
    "\\theta := \\theta - \\alpha \\left( \\frac{1}{m} X^T (X\\theta - y) + \\lambda \\cdot \\text{sign}(\\theta) \\right)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate.\n",
    "- $X$ is the matrix of input features.\n",
    "\n",
    "This indicates that the update for $\\theta$ in Lasso regression includes a shrinkage term that penalizes the absolute value of the coefficients. Unlike Ridge regression, where the penalty is quadratic (squared values of $\\theta_j$), the Lasso penalty is linear, leading to sparsity in the model (some coefficients can become exactly zero). This makes Lasso particularly useful for feature selection in high-dimensional datasets."
   ],
   "id": "a3bc0b0d2660366c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
